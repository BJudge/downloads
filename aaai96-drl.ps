%!PS-Adobe-3.0
%%Title: (DRL-final)
%%Creator: (Microsoft Word: LaserWriter 8 8.2)
%%CreationDate: (10:47 AM Wednesday, May 15, 1996)
%%For: ()
%%Pages: 6
%%DocumentFonts: Times-Roman Symbol Times-Bold Times-Italic
%%DocumentNeededFonts: Times-Roman Symbol Times-Bold Times-Italic
%%DocumentSuppliedFonts:
%%DocumentData: Clean7Bit
%%PageOrder: Ascend
%%Orientation: Portrait
%%DocumentMedia: Default 612 792 0 () ()
%ADO_ImageableArea: 30 31 582 761
%%EndComments
userdict begin/dscInfo 5 dict dup begin
/Title(DRL-final)def
/Creator(Microsoft Word: LaserWriter 8 8.2)def
/CreationDate(10:47 AM Wednesday, May 15, 1996)def
/For()def
/Pages 1 def
end def end
/md 200 dict def md begin/currentpacking where {pop /sc_oldpacking currentpacking def true setpacking}if
%%BeginFile: adobe_psp_basic
%%Copyright: Copyright 1990-1993 Adobe Systems Incorporated. All Rights Reserved.
/bd{bind def}bind def
/xdf{exch def}bd
/xs{exch store}bd
/ld{load def}bd
/Z{0 def}bd
/T/true
/F/false
/:L/lineto
/lw/setlinewidth
/:M/moveto
/rl/rlineto
/rm/rmoveto
/:C/curveto
/:T/translate
/:K/closepath
/:mf/makefont
/gS/gsave
/gR/grestore
/np/newpath
14{ld}repeat
/$m matrix def
/av 81 def
/por true def
/normland false def
/psb-nosave{}bd
/pse-nosave{}bd
/us Z
/psb{/us save store}bd
/pse{us restore}bd
/level2
/languagelevel where
{
pop languagelevel 2 ge
}{
false
}ifelse
def
/featurecleanup
{
stopped
cleartomark
countdictstack exch sub dup 0 gt
{
{end}repeat
}{
pop
}ifelse
}bd
/noload Z
/startnoload
{
{/noload save store}if
}bd
/endnoload
{
{noload restore}if
}bd
level2 startnoload
/setjob
{
statusdict/jobname 3 -1 roll put
}bd
/setcopies
{
userdict/#copies 3 -1 roll put
}bd
level2 endnoload level2 not startnoload
/setjob
{
1 dict begin/JobName xdf currentdict end setuserparams
}bd
/setcopies
{
1 dict begin/NumCopies xdf currentdict end setpagedevice
}bd
level2 not endnoload
/pm Z
/mT Z
/sD Z
/realshowpage Z
/initializepage
{
/pm save store mT concat
}bd
/endp
{
pm restore showpage
}def
/$c/DeviceRGB def
/rectclip where
{
pop/rC/rectclip ld
}{
/rC
{
np 4 2 roll
:M
1 index 0 rl
0 exch rl
neg 0 rl
:K
clip np
}bd
}ifelse
/rectfill where
{
pop/rF/rectfill ld
}{
/rF
{
gS
np
4 2 roll
:M
1 index 0 rl
0 exch rl
neg 0 rl
fill
gR
}bd
}ifelse
/rectstroke where
{
pop/rS/rectstroke ld
}{
/rS
{
gS
np
4 2 roll
:M
1 index 0 rl
0 exch rl
neg 0 rl
:K
stroke
gR
}bd
}ifelse
%%EndFile
%%BeginFile: adobe_psp_colorspace_level1
%%Copyright: Copyright 1991-1993 Adobe Systems Incorporated. All Rights Reserved.
/G/setgray ld
/:F/setrgbcolor ld
%%EndFile
level2 startnoload
%%BeginFile: adobe_psp_patterns_level1
%%Copyright: Copyright 1991-1993 Adobe Systems Incorporated. All Rights Reserved.
/patfreq Z
/patangle Z
/bk Z
/fg Z
/docolorscreen Z
/graystring Z
/pattransf{}def
/initQDpatterns
{
/patfreq 9.375 store
/patangle
1 0 $m defaultmatrix dtransform
exch atan
por not
{90 add}if
normland{180 add}if
store
:a
}def
/docolorscreen
/setcolorscreen where
{
pop/currentcolorscreen where
{
pop/setcmykcolor where
{
pop true
}{
false
}ifelse
}{
false
}ifelse
}{
false
}ifelse
def
/setgraypattern
{
/graystring xs
patfreq
patangle
{
1 add
4 mul
cvi
graystring
exch get
exch
1 add 4 mul
cvi
7 sub
bitshift
1 and
}setscreen
64 div setgray
}bd
/:b
{
/pattransf load settransfer
pop pop pop
setgraypattern
}bd
docolorscreen startnoload
/screensave 5 array def
/:a{currentgray currentscreen currenttransfer screensave astore pop}bd
/:e{screensave aload pop settransfer setscreen setgray}bd
/:d
{
pop pop pop
/pattransf load settransfer
setgraypattern 8{pop}repeat
}bd
/:c
/:d ld
docolorscreen endnoload docolorscreen not startnoload
/screensave 20 array def
/:a{currentcmykcolor currentcolorscreen currentcolortransfer screensave astore pop}bd
/:e{screensave aload pop setcolortransfer setcolorscreen setcmykcolor}bd
/rstring Z
/grstring Z
/blstring Z
/convroll{64 div 4 -1 roll}bd
/setcolorpattern
{
/graystring xs
/blstring xs
/grstring xs
/rstring xs
patfreq
patangle
{
1 add 4 mul cvi rstring
exch get exch 1 add 4 mul
cvi 7 sub bitshift 1 and
}
patfreq
patangle
{
1 add 4 mul cvi grstring
exch get exch 1 add 4 mul
cvi 7 sub bitshift 1 and
}
patfreq
patangle
{
1 add 4 mul cvi blstring
exch get exch 1 add 4 mul
cvi 7 sub bitshift 1 and
}
patfreq
patangle
{
1 add 4 mul cvi graystring
exch get exch 1 add 4 mul
cvi 7 sub bitshift 1 and
}
setcolorscreen
convroll convroll convroll convroll
setcmykcolor
}bd
/:d
{
pop pop pop
/pattransf load settransfer
pop pop setcolorpattern
}bd
/:c
/:d ld
docolorscreen not endnoload
%%EndFile
level2  endnoload level2 not startnoload
%%BeginFile: adobe_psp_patterns_level2
%%Copyright: Copyright 1990-1993 Adobe Systems Incorporated. All Rights Reserved.
/pmtx Z
/BGnd Z
/FGnd Z
/PaintData Z
/PatternMtx Z
/PatHeight Z
/PatWidth Z
/$d Z
/savecolor 4 array def
/savecolorspace Z
/:a{
mark 0 0 0 currentcolor savecolor astore pop cleartomark
/savecolorspace currentcolorspace store
}bd
/:e{
savecolorspace setcolorspace
mark savecolor aload pop setcolor cleartomark
}bd
/initQDpatterns
{
gS
initmatrix
mT dup 4 get exch 5 get :T
1 0 dtransform round exch round exch idtransform
dup mul exch dup mul exch add sqrt
0 1 dtransform round exch round exch idtransform
dup mul exch dup mul exch add sqrt
neg
scale
0
por not{90 add}if
normland{180 add}if
rotate
matrix currentmatrix
gR
/pmtx xs
:a
}bd
/:t
{
14 dict begin
/BGnd xdf
/FGnd xdf
/PaintData xdf
/PatternType 1 def
/PaintType 1 def
/BBox[0 0 1 1]def
/TilingType 1 def
/XStep 1 def
/YStep 1 def
/PatternMtx[24 0 0 24 0 0]def
/PaintProc
BGnd null ne
{
{
begin
BGnd aload pop :F
0 0 1 1 rF
FGnd aload pop :F
24 24 true PatternMtx PaintData imagemask
end
}
}{
{
begin
FGnd aload pop :F
24 24 true PatternMtx PaintData imagemask
end
}
}ifelse
def
currentdict
PatternMtx
end
gS $c setcolorspace pmtx setmatrix makepattern gR
}bd
/:u
{
14 dict begin
/$d 8 dict def
/PatternType 1 def
/PaintType 1 def
/BBox[0 0 1 1]def
/TilingType 1 def
/XStep 1 def
/YStep 1 def
/PaintData xdf
/PatHeight xdf
/PatWidth xdf
/PatternMtx[PatWidth 0 0 PatHeight 0 0]def
$d begin
/ImageType 1 def
/MultipleDataSource false def
/Height PatHeight def
/Width PatWidth def
/Decode[0 1 0 1 0 1]def
/ImageMatrix PatternMtx def
/DataSource PaintData def
/BitsPerComponent 8 def
end
/PaintProc
{
begin
$d image
end
}def
currentdict
PatternMtx
end
gS $c setcolorspace pmtx setmatrix makepattern gR
}bd
/bk[1 1 1]def
/fg[0 0 0]def
/:b{
:t
setpattern
pop pop
}bd
/:d{
:t
setpattern
10{pop}repeat
}bd
/:c{
:u
setpattern
10{pop}repeat
}bd
%%EndFile
level2 not endnoload
%%BeginFile: adobe_psp_uniform_graphics
%%Copyright: Copyright 1990-1993 Adobe Systems Incorporated. All Rights Reserved.
/@a
{
np :M 0 rl :L 0 exch rl 0 rl :L fill
}bd
/@b
{
np :M 0 rl 0 exch rl :L 0 rl 0 exch rl fill
}bd
/arct where
{
pop
}{
/arct
{
arcto pop pop pop pop
}bd
}ifelse
/x1 Z
/x2 Z
/y1 Z
/y2 Z
/rad Z
/@q
{
/rad xs
/y2 xs
/x2 xs
/y1 xs
/x1 xs
np
x2 x1 add 2 div y1 :M
x2 y1 x2 y2 rad arct
x2 y2 x1 y2 rad arct
x1 y2 x1 y1 rad arct
x1 y1 x2 y1 rad arct
fill
}bd
/@s
{
/rad xs
/y2 xs
/x2 xs
/y1 xs
/x1 xs
np
x2 x1 add 2 div y1 :M
x2 y1 x2 y2 rad arct
x2 y2 x1 y2 rad arct
x1 y2 x1 y1 rad arct
x1 y1 x2 y1 rad arct
:K
stroke
}bd
/@i
{
np 0 360 arc fill
}bd
/@j
{
gS
np
:T
scale
0 0 .5 0 360 arc
fill
gR
}bd
/@e
{
np
0 360 arc
:K
stroke
}bd
/@f
{
np
$m currentmatrix
pop
:T
scale
0 0 .5 0 360 arc
:K
$m setmatrix
stroke
}bd
/@k
{
gS
np
:T
0 0 :M
0 0 5 2 roll
arc fill
gR
}bd
/@l
{
gS
np
:T
0 0 :M
scale
0 0 .5 5 -2 roll arc
fill
gR
}bd
/@m
{
np
arc
stroke
}bd
/@n
{
np
$m currentmatrix
pop
:T
scale
0 0 .5 5 -2 roll arc
$m setmatrix
stroke
}bd
%%EndFile
%%BeginFile: adobe_psp_basic_text
%%Copyright: Copyright 1990-1993 Adobe Systems Incorporated. All Rights Reserved.
/S/show ld
/A{
0.0 exch ashow
}bd
/R{
0.0 exch 32 exch widthshow
}bd
/W{
0.0 3 1 roll widthshow
}bd
/J{
0.0 32 4 2 roll 0.0 exch awidthshow
}bd
/V{
0.0 4 1 roll 0.0 exch awidthshow
}bd
/fcflg true def
/fc{
fcflg{
vmstatus exch sub 50000 lt{
(%%[ Warning: Running out of memory ]%%\r)print flush/fcflg false store
}if pop
}if
}bd
/$f[1 0 0 -1 0 0]def
/:ff{$f :mf}bd
/MacEncoding StandardEncoding 256 array copy def
MacEncoding 39/quotesingle put
MacEncoding 96/grave put
/Adieresis/Aring/Ccedilla/Eacute/Ntilde/Odieresis/Udieresis/aacute
/agrave/acircumflex/adieresis/atilde/aring/ccedilla/eacute/egrave
/ecircumflex/edieresis/iacute/igrave/icircumflex/idieresis/ntilde/oacute
/ograve/ocircumflex/odieresis/otilde/uacute/ugrave/ucircumflex/udieresis
/dagger/degree/cent/sterling/section/bullet/paragraph/germandbls
/registered/copyright/trademark/acute/dieresis/notequal/AE/Oslash
/infinity/plusminus/lessequal/greaterequal/yen/mu/partialdiff/summation
/product/pi/integral/ordfeminine/ordmasculine/Omega/ae/oslash
/questiondown/exclamdown/logicalnot/radical/florin/approxequal/Delta/guillemotleft
/guillemotright/ellipsis/space/Agrave/Atilde/Otilde/OE/oe
/endash/emdash/quotedblleft/quotedblright/quoteleft/quoteright/divide/lozenge
/ydieresis/Ydieresis/fraction/currency/guilsinglleft/guilsinglright/fi/fl
/daggerdbl/periodcentered/quotesinglbase/quotedblbase/perthousand
/Acircumflex/Ecircumflex/Aacute/Edieresis/Egrave/Iacute/Icircumflex/Idieresis/Igrave
/Oacute/Ocircumflex/apple/Ograve/Uacute/Ucircumflex/Ugrave/dotlessi/circumflex/tilde
/macron/breve/dotaccent/ring/cedilla/hungarumlaut/ogonek/caron
MacEncoding 128 128 getinterval astore pop
level2 startnoload
/copyfontdict
{
findfont dup length dict
begin
{
1 index/FID ne{def}{pop pop}ifelse
}forall
}bd
level2 endnoload level2 not startnoload
/copyfontdict
{
findfont dup length dict
copy
begin
}bd
level2 not endnoload
md/fontname known not{
/fontname/customfont def
}if
/Encoding Z
/:mre
{
copyfontdict
/Encoding MacEncoding def
fontname currentdict
end
definefont :ff def
}bd
/:bsr
{
copyfontdict
/Encoding Encoding 256 array copy def
Encoding dup
}bd
/pd{put dup}bd
/:esr
{
pop pop
fontname currentdict
end
definefont :ff def
}bd
/scf
{
scalefont def
}bd
/scf-non
{
$m scale :mf setfont
}bd
/ps Z
/fz{/ps xs}bd
/sf/setfont ld
/cF/currentfont ld
/mbf
{
/makeblendedfont where
{
pop
makeblendedfont
/ABlend exch definefont
}{
pop
}ifelse
def
}def
%%EndFile
%%BeginFile: adobe_psp_derived_styles
%%Copyright: Copyright 1990-1993 Adobe Systems Incorporated. All Rights Reserved.
/wi
version(23.0)eq
{
{
gS 0 0 0 0 rC stringwidth gR
}bind
}{
/stringwidth load
}ifelse
def
/$o 1. def
/gl{$o G}bd
/ms{:M S}bd
/condensedmtx[.82 0 0 1 0 0]def
/:mc
{
condensedmtx :mf def
}bd
/extendedmtx[1.18 0 0 1 0 0]def
/:me
{
extendedmtx :mf def
}bd
/basefont Z
/basefonto Z
/dxa Z
/dxb Z
/dxc Z
/dxd Z
/dsdx2 Z
/bfproc Z
/:fbase
{
dup/FontType get 0 eq{
dup length dict begin
dup{1 index/FID ne 2 index/UniqueID ne and{def}{pop pop}ifelse}forall
/FDepVector exch/FDepVector get[exch/:fbase load forall]def
}/bfproc load ifelse
/customfont currentdict end definefont
}bd
/:mo
{
/bfproc{
dup dup length 2 add dict
begin
{
1 index/FID ne 2 index/UniqueID ne and{def}{pop pop}ifelse
}forall
/PaintType 2 def
/StrokeWidth .012 0 FontMatrix idtransform pop def
/customfont currentdict
end
definefont
8 dict begin
/basefonto xdf
/basefont xdf
/FontType 3 def
/FontMatrix[1 0 0 1 0 0]def
/FontBBox[0 0 1 1]def
/Encoding StandardEncoding def
/BuildChar
{
exch begin
basefont setfont
( )dup 0 4 -1 roll put
dup wi
setcharwidth
0 0 :M
gS
gl
dup show
gR
basefonto setfont
show
end
}def
}store :fbase
}bd
/:mso
{
/bfproc{
7 dict begin
/basefont xdf
/FontType 3 def
/FontMatrix[1 0 0 1 0 0]def
/FontBBox[0 0 1 1]def
/Encoding StandardEncoding def
/BuildChar
{
exch begin
sD begin
/dxa 1 ps div def
basefont setfont
( )dup 0 4 -1 roll put
dup wi
1 index 0 ne
{
exch dxa add exch
}if
setcharwidth
dup 0 0 ms
dup dxa 0 ms
dup dxa dxa ms
dup 0 dxa ms
gl
dxa 2. div dup ms
end
end
}def
}store :fbase
}bd
/:ms
{
/bfproc{
dup dup length 2 add dict
begin
{
1 index/FID ne 2 index/UniqueID ne and{def}{pop pop}ifelse
}forall
/PaintType 2 def
/StrokeWidth .012 0 FontMatrix idtransform pop def
/customfont currentdict
end
definefont
8 dict begin
/basefonto xdf
/basefont xdf
/FontType 3 def
/FontMatrix[1 0 0 1 0 0]def
/FontBBox[0 0 1 1]def
/Encoding StandardEncoding def
/BuildChar
{
exch begin
sD begin
/dxb .05 def
basefont setfont
( )dup 0 4 -1 roll put
dup wi
exch dup 0 ne
{
dxb add
}if
exch setcharwidth
dup dxb .01 add 0 ms
0 dxb :T
gS
gl
dup 0 0 ms
gR
basefonto setfont
0 0 ms
end
end
}def
}store :fbase
}bd
/:mss
{
/bfproc{
7 dict begin
/basefont xdf
/FontType 3 def
/FontMatrix[1 0 0 1 0 0]def
/FontBBox[0 0 1 1]def
/Encoding StandardEncoding def
/BuildChar
{
exch begin
sD begin
/dxc 1 ps div def
/dsdx2 .05 dxc 2 div add def
basefont setfont
( )dup 0 4 -1 roll put
dup wi
exch dup 0 ne
{
dsdx2 add
}if
exch setcharwidth
dup dsdx2 .01 add 0 ms
0 .05 dxc 2 div sub :T
dup 0 0 ms
dup dxc 0 ms
dup dxc dxc ms
dup 0 dxc ms
gl
dxc 2 div dup ms
end
end
}def
}store :fbase
}bd
/:msb
{
/bfproc{
7 dict begin
/basefont xdf
/FontType 3 def
/FontMatrix[1 0 0 1 0 0]def
/FontBBox[0 0 1 1]def
/Encoding StandardEncoding def
/BuildChar
{
exch begin
sD begin
/dxd .03 def
basefont setfont
( )dup 0 4 -1 roll put
dup wi
1 index 0 ne
{
exch dxd add exch
}if
setcharwidth
dup 0 0 ms
dup dxd 0 ms
dup dxd dxd ms
0 dxd ms
end
end
}def
}store :fbase
}bd
/italicmtx[1 0 -.212557 1 0 0]def
/:mi
{
italicmtx :mf def
}bd
/:v
{
[exch dup/FontMatrix get exch
dup/FontInfo known
{
/FontInfo get
dup/UnderlinePosition known
{
dup/UnderlinePosition get
2 index 0
3 1 roll
transform
exch pop
}{
.1
}ifelse
3 1 roll
dup/UnderlineThickness known
{
/UnderlineThickness get
exch 0 3 1 roll
transform
exch pop
abs
}{
pop pop .067
}ifelse
}{
pop pop .1 .067
}ifelse
]
}bd
/$t Z
/$p Z
/$s Z
/:p
{
aload pop
2 index mul/$t xs
1 index mul/$p xs
.012 mul/$s xs
}bd
/:m
{gS
0 $p rm
$t lw
0 rl stroke
gR
}bd
/:n
{
gS
0 $p rm
$t lw
0 rl
gS
gl
stroke
gR
strokepath
$s lw
/setstrokeadjust where{pop
currentstrokeadjust true setstrokeadjust stroke setstrokeadjust
}{
stroke
}ifelse
gR
}bd
/:o
{gS
0 $p rm
$t 2 div dup rm
$t lw
dup 0 rl
stroke
gR
:n
}bd
%%EndFile
/currentpacking where {pop sc_oldpacking setpacking}if end
%%EndProlog
%%BeginSetup
md begin
countdictstack[{
%%BeginFeature: *ManualFeed False
level2 {1 dict dup /ManualFeed false put setpagedevice}{statusdict begin /manualfeed false store end} ifelse
%%EndFeature
}featurecleanup
countdictstack[{
%%BeginFeature: *InputSlot Upper

%%EndFeature
}featurecleanup
countdictstack[{
%%BeginFeature: *PageRegion LetterSmall
level2 {
		2 dict dup /PageSize [612 792] put dup /ImagingBBox [30 31 582 761] put setpagedevice
	}{
		/lettersmall where {pop lettersmall} {letterR} ifelse
	} ifelse
%%EndFeature
}featurecleanup
()setjob
/mT[1 0 0 -1 30 761]def
initQDpatterns
/sD 16 dict def
300 level2{1 dict dup/WaitTimeout 4 -1 roll put setuserparams}{statusdict/waittimeout 3 -1 roll put}ifelse
%%IncludeFont: Times-Roman
%%IncludeFont: Symbol
%%IncludeFont: Times-Bold
%%IncludeFont: Times-Italic
/f0_1/Times-Roman
:mre
/f0_18 f0_1 18 scf
/f0_10 f0_1 10 scf
/f0_9 f0_1 9 scf
/f0_7 f0_1 7 scf
/f1_1/Symbol
:bsr
240/apple pd
:esr
/f1_10 f1_1 10 scf
/f2_1/Times-Bold
:mre
/f2_12 f2_1 12 scf
/f2_11 f2_1 11 scf
/f2_10 f2_1 10 scf
/f3_1/Times-Italic
:mre
/f3_10 f3_1 10 scf
/f4_1 f1_1
:mi
/f4_10 f4_1 10 scf
/Courier findfont[10 0 0 -10 0 0]:mf setfont
%%EndSetup
%%Page: 1 1
%%BeginPageSetup
initializepage
(; page: 1 of 6)setjob
%%EndPageSetup
gS 0 0 552 730 rC
54 61 :M
f0_18 sf
(Scaling Up: Distributed Machine Learning with Cooperation)S
492 54 :M
f0_10 sf
(*)S
121 88 :M
f2_12 sf
(Foster John Provost)S
108 101 :M
f0_10 sf
(NYNEX Science & Technology)S
123 113 :M
(400 Westchester Avenue)S
124 125 :M
(White Plains, NY 10604)S
131 137 :M
(foster@nynexst.com)S
325 88 :M
f2_12 sf
(Daniel N. Hennessy)S
314 101 :M
f0_10 sf
(Computer Science Department)S
327 113 :M
(University of Pittsburgh)S
332 125 :M
(Pittsburgh, PA 15260)S
332 137 :M
(hennessy@cs.pitt.edu)S
125 161 :M
f2_10 sf
(Abstract)S
34 172 :M
f0_10 sf
.341 .034(Machine-learning methods are becoming increasingly)J
34 183 :M
2.775 .278(popular for automated data analysis.  However,)J
34 194 :M
.101 .01(standard methods do not scale up to massive scientific)J
34 205 :M
1.22 .122(and business data sets without expensive hardware.)J
34 216 :M
2.146 .215(This paper investigates a practical alternative for)J
34 227 :M
1.027 .103(scaling up: the use of distributed processing to take)J
34 238 :M
.367 .037(advantage of the often dormant PCs and workstations)J
34 249 :M
.377 .038(available on local networks.  Each workstation runs a)J
34 260 :M
1.769 .177(common rule-learning program on a subset of the)J
34 271 :M
1.422 .142(data.  We first show that for commonly used rule)J
250 271 :M
(-)S
34 282 :M
.705 .071(evaluation criteria, a simple form of cooperation can)J
34 293 :M
1.805 .181(guarantee that a rule will look good to the set of)J
34 304 :M
.308 .031(cooperating learners if and only if it would look good)J
34 315 :M
.79 .079(to a single learner operating with the entire data set.)J
34 326 :M
3.096 .31(We then show how such a system can further)J
34 337 :M
.22 .022(capitalize on different perspectives by sharing learned)J
34 348 :M
.892 .089(knowledge for significant reduction in search effort.)J
34 359 :M
.407 .041(We demonstrate the power of the method by learning)J
34 370 :M
1.498 .15(from a massive data set taken from the domain of)J
34 381 :M
2.399 .24(cellular fraud detection.  Finally, we provide an)J
34 392 :M
1.469 .147(overview of other methods for scaling up machine)J
34 403 :M
(learning.)S
111 435 :M
f2_12 sf
(Introduction)S
24 453 :M
f0_10 sf
2.256 .226(Machine-learning techniques are prime candidates for)J
24 464 :M
1.376 .138(automated analysis of large business and scientific data)J
24 475 :M
1.661 .166(sets. Large data sets are necessary for higher accuracy)J
24 486 :M
3.707 .371(\(Catlett, 1991b\), for learning small disjuncts with)J
24 497 :M
-.014(confidence, and to avoid over-fitting with large feature sets.)A
24 508 :M
2.574 .257(However, the standard tools of the machine-learning)J
24 519 :M
2.017 .202(researcher, such as off-the-shelf learning programs on)J
24 530 :M
.072 .007(workstation platforms, do not scale up to massive data sets.)J
24 541 :M
1.207 .121(For example, Catlett estimates that ID3 \(Quinlan, 1986\))J
24 552 :M
.455 .046(would take several months to learn from a million records)J
24 563 :M
(in the flight data set from NASA \(Catlett, 1991a\).)S
34 574 :M
.361 .036(One solution to this scaling problem is to invest in or to)J
24 585 :M
1.741 .174(gain access to very powerful hardware.  Another is to)J
24 596 :M
2.707 .271(design alternative methods that can deal better with)J
24 607 :M
1.369 .137(massive data sets.  In this paper, we investigate a third)J
24 618 :M
.521 .052(solution, namely, to take advantage of existing processing)J
-4126 -4126 -1 1 -4124 -4126 1 -4126 -4127 @a
24 629.24 -.24 .24 167.24 629 .24 24 629 @a
34 643 :M
f0_7 sf
1.157 .116( )J
f0_9 sf
0 -1 rm
(*)S
0 1 rm
45 645 :M
f0_10 sf
2.181 .218(First published in )J
f3_10 sf
2.961 .296(Proceedings of the National)J
24 656 :M
.71 .071(Conference on Artificial Intelligence. Menlo Park, CA:)J
24 667 :M
(AAAI Press, 1996.)S
289 160 :M
f0_10 sf
.629 .063(power distributed across a local network and often under)J
525 160 :M
(-)S
289 171 :M
.641 .064(utilized.  In particular, we focus on partitioning the set of)J
289 182 :M
.631 .063(examples and distributing the subsets across a network of)J
289 193 :M
2.108 .211(heterogeneous workstations.  We use a standard rule-)J
289 204 :M
.518 .052(learning algorithm, modified slightly to allow cooperation)J
289 215 :M
(between learners.)S
299 226 :M
.473 .047(At a high level, our metaphor for distributed learning is)J
289 237 :M
1.091 .109(one of cooperating experts, each of which has a slightly)J
289 248 :M
1.37 .137(different perspective on the concept to be learned.  We)J
289 259 :M
3.961 .396(define )J
323 259 :M
f3_10 sf
.805(cooperation)A
f0_10 sf
2.819 .282( as the learning-time sharing of)J
289 270 :M
3.639 .364(information to increase the quality of the learned)J
289 281 :M
.037 .004(knowledge or to reduce or redirect the search.  The learners)J
289 292 :M
.924 .092(communicate with each other by passing messages.  The)J
289 303 :M
.392 .039(group can take advantage of the communication by asking)J
289 314 :M
(questions or by sharing learned knowledge.)S
299 325 :M
1.339 .134(We present a practical method for scaling up to very)J
289 336 :M
2.465 .246(large data sets that can be guaranteed to learn rules)J
289 347 :M
1.615 .162(equivalent to those learned by a )J
f3_10 sf
.552(monolithic)A
481 347 :M
f0_10 sf
2.221 .222( learner, a)J
289 358 :M
.244 .024(learner operating on the entire data set.  In the next section)J
289 369 :M
.087 .009(we show how to guarantee that every rule that a monolithic)J
289 380 :M
.608 .061(learner would judge to be satisfactory would appear to be)J
289 391 :M
.103 .01(satisfactory to at least one of our distributed learners.  Next)J
289 402 :M
-.005(we discuss how distributed rule learners can take advantage)A
289 413 :M
.224 .022(of this property by cooperating to ensure that the ensemble)J
289 424 :M
.889 .089(learns )J
317 424 :M
f3_10 sf
.246(only)A
f0_10 sf
.812 .081( rules that are satisfactory over the entire data)J
289 435 :M
2.576 .258(set. We present results demonstrating the distributed)J
289 446 :M
.597 .06(system's ability to scale up when learning from a massive)J
289 457 :M
1.671 .167(set of cellular fraud data.  Later we show how further)J
289 468 :M
.657 .066(cooperation can increase the scaling substantially. Finally)J
289 479 :M
(we discuss other approaches to scaling up.)S
314 511 :M
f2_12 sf
(Partitioning and Accuracy Estimates)S
289 529 :M
f0_10 sf
2.89 .289(If learning algorithms had access to the probability)J
289 540 :M
3.382 .338(distribution over the example space, then a useful)J
289 551 :M
1.414 .141(definition of the quality of a learned rule would be the)J
289 562 :M
1.125 .112(probability that the class indicated by the rule is correct)J
289 573 :M
.84 .084(when its conditions apply to an example.  Unfortunately,)J
289 584 :M
.491 .049(the probability distribution is not usually available.  Thus,)J
289 595 :M
.071 .007(statistics from the training set typically are used to estimate)J
289 606 :M
2.58 .258(the probability that a rule is correct.  The )J
f3_10 sf
.932(positive)A
289 617 :M
.949 .095(predictive value)J
356 617 :M
f0_10 sf
1.575 .158( discussed by Weiss, )J
f3_10 sf
1.502 .15(et al.)J
473 617 :M
f0_10 sf
1.695 .169( \(1990\), is a)J
289 628 :M
-.004(frequency-based accuracy estimate; the rule certainty factor)A
289 639 :M
1.745 .174(used by Quinlan \(1987\) is a frequency-based accuracy)J
289 650 :M
2.028 .203(estimate adjusted for small samples, and several rule-)J
289 661 :M
2.45 .245(learning programs use the Laplace accuracy estimate)J
endp
%%Page: 2 2
%%BeginPageSetup
initializepage
(; page: 2 of 6)setjob
%%EndPageSetup
-30 -31 :T
gS 30 31 552 730 rC
54 63 :M
f0_10 sf
1.267 .127(\(Clark & Boswell, 1991; Segal & Etzioni, 1994; Webb,)J
54 74 :M
.583 .058(1995; Quinlan & Cameron-Jones, 1995\).  We show that a)J
54 85 :M
.557 .056(distributed learner can make performance guarantees with)J
54 96 :M
(respect to each of these rule quality metrics.)S
64 107 :M
.216 .022(It is useful to begin by defining some terms. A rule, )J
277 107 :M
f3_10 sf
(r)S
281 107 :M
f0_10 sf
.246 .025(, is)J
54 118 :M
.507 .051(a class description that will either cover or not cover each)J
54 129 :M
.848 .085(example in a data set, )J
149 129 :M
f3_10 sf
.271(E)A
f0_10 sf
.703 .07(. Thus, coverage statistics can be)J
54 140 :M
.979 .098(determined for )J
119 140 :M
f3_10 sf
(r)S
123 140 :M
f0_10 sf
1.387 .139( and )J
146 140 :M
f3_10 sf
.775(E)A
f0_10 sf
1.016 .102(. Let )J
f3_10 sf
.775(P)A
f0_10 sf
1.027 .103( and )J
205 140 :M
f3_10 sf
.548(N)A
f0_10 sf
1.072 .107( be the numbers of)J
54 151 :M
.797 .08(positive and negative examples in )J
199 151 :M
f3_10 sf
.365(E)A
f0_10 sf
.836 .084(. The number of true)J
54 162 :M
.181 .018(positives, )J
f3_10 sf
.109(TP)A
107 162 :M
f0_10 sf
.322 .032(, and the number of false positives, )J
253 162 :M
f3_10 sf
.076(FP)A
f0_10 sf
.211 .021(, count)J
54 173 :M
1.294 .129(the positive and negative examples covered by )J
f3_10 sf
(r)S
262 173 :M
f0_10 sf
1.683 .168(. For a)J
54 184 :M
2.725 .273(subset, )J
89 184 :M
f3_10 sf
(E)S
0 2 rm
(i)S
0 -2 rm
98 184 :M
f0_10 sf
3.267 .327(, of )J
f3_10 sf
3.152(E)A
f0_10 sf
2.345 .235(, )J
139 184 :M
f3_10 sf
(T)S
0 2 rm
(i)S
0 -2 rm
147 184 :M
f0_10 sf
4.212 .421(, )J
157 184 :M
f3_10 sf
(N)S
0 2 rm
(i)S
0 -2 rm
166 184 :M
f0_10 sf
4.212 .421(, )J
176 184 :M
f3_10 sf
(TP)S
0 2 rm
(i)S
0 -2 rm
190 184 :M
f0_10 sf
2.117 .212(, and )J
f3_10 sf
1.533(FP)A
0 2 rm
(i)S
0 -2 rm
236 184 :M
f0_10 sf
2.989 .299( are defined)J
54 196 :M
(analogously.)S
64 207 :M
2.555 .256(Let us define a )J
140 207 :M
f3_10 sf
2.838 .284(rule evaluation criterion)J
f0_10 sf
1.23 .123( to be the)J
54 218 :M
1.013 .101(combination of a )J
f3_10 sf
1.515 .151(rule evaluation function)J
232 218 :M
f0_10 sf
.209(,)A
f3_10 sf
1.279 .128( f\(r,E\))J
261 218 :M
f0_10 sf
1.409 .141(, which)J
54 229 :M
1.7 .17(takes a rule and an example set and produces a scalar)J
54 240 :M
.88 .088(evaluation score, and a )J
154 240 :M
f3_10 sf
.136(threshold)A
f0_10 sf
.147 .015(, )J
198 240 :M
f3_10 sf
(c.)S
205 240 :M
f0_10 sf
.99 .099(  With respect to the)J
54 251 :M
.862 .086(rule evaluation criterion, a rule, )J
190 251 :M
f3_10 sf
(r)S
194 251 :M
f0_10 sf
.353 .035(, is )J
f3_10 sf
.242(satisfactory)A
f0_10 sf
.743 .074( over an)J
54 262 :M
(example set, )S
f3_10 sf
(E)S
113 262 :M
f0_10 sf
(, if )S
f3_10 sf
(f\(r,E\) )S
151 262 :M
f4_10 sf
<B320>S
159 262 :M
f3_10 sf
(c)S
f0_10 sf
-.003(.  Rule evaluation criteria can be)A
54 273 :M
.04 .004(defined for each of the three rule quality metrics referenced)J
54 284 :M
.237 .024(above by defining the appropriate rule evaluation function.)J
54 295 :M
2.843 .284(For positive predictive value, )J
194 295 :M
f3_10 sf
3.373 .337(f\(r,E\) =  ppv\(r,E\) =)J
54 306 :M
(TP/\(TP+FP\))S
106 306 :M
f0_10 sf
1.9 .19(; for the certainty factor used by Quinlan,)J
54 317 :M
f3_10 sf
2.454 .245(f\(r,E\) = cf\(r,E\) = \(TP-0.5\)/\(TP+FP\))J
f0_10 sf
1.803 .18(; for the Laplace)J
54 328 :M
.574 .057(accuracy estimate, )J
133 328 :M
f3_10 sf
.572 .057(f\(r,E\) = le\(r,E\) = \(TP+1\)/\(TP+FP+k\))J
f0_10 sf
(,)S
54 339 :M
(where )S
81 339 :M
f3_10 sf
(k)S
f0_10 sf
( is the number of classes in the data.)S
64 350 :M
.764 .076(Given a set of examples, )J
f3_10 sf
.346(E)A
f0_10 sf
.608 .061(, and a partition of )J
f3_10 sf
.346(E)A
f0_10 sf
.466 .047( into )J
286 350 :M
f3_10 sf
(N)S
54 361 :M
f0_10 sf
2.278 .228(disjoint subsets, )J
f3_10 sf
.797(E)A
0 2 rm
(i)S
0 -2 rm
139 361 :M
f0_10 sf
4.462 .446(, )J
149 361 :M
f3_10 sf
.934(i=1..N)A
f0_10 sf
1.686 .169(, the )J
205 361 :M
f3_10 sf
(invariant-partitioning)S
54 373 :M
.035(property)A
f0_10 sf
.127 .013( \(introduced by Provost & Hennessy \(1994\)\) is the)J
54 384 :M
1.957 .196(phenomenon that for some rule evaluation criteria the)J
54 395 :M
2.09 .209(following holds for all partitions of )J
218 395 :M
f3_10 sf
1.69(E)A
f0_10 sf
2.068 .207(: if a rule )J
f3_10 sf
(r)S
280 395 :M
f0_10 sf
2.816 .282( is)J
54 406 :M
1.104 .11(satisfactory over )J
127 406 :M
f3_10 sf
.689(E)A
f0_10 sf
1.323 .132(, then there exists an )J
228 406 :M
f3_10 sf
.391(i)A
f0_10 sf
1.434 .143( such that )J
277 406 :M
f3_10 sf
(r)S
281 406 :M
f0_10 sf
1.702 .17( is)J
54 417 :M
1.69 .169(satisfactory over )J
f3_10 sf
.57(E)A
0 2 rm
(i)S
0 -2 rm
139 417 :M
f0_10 sf
2.158 .216(. The implication of the invariant-)J
54 429 :M
.251 .025(partitioning property is that distributed learning algorithms)J
54 440 :M
.264 .026(can be designed such that each processor has only a subset)J
54 451 :M
1.289 .129(of the entire set of examples, but every rule that would)J
54 462 :M
1.837 .184(appear satisfactory to a monolithic learner will appear)J
54 473 :M
2.901 .29(satisfactory to at least one distributed learner. It is)J
54 484 :M
2.187 .219(straightforward to show that the invariant-partitioning)J
54 495 :M
(property holds for positive predictive value.)S
64 506 :M
.281 .028(Unfortunately, it is also straightforward to show that the)J
54 517 :M
.296 .03(property does not hold for the rule certainty factor used by)J
54 528 :M
.144 .014(Quinlan or for the Laplace accuracy estimate. However, by)J
54 539 :M
.925 .092(extending the property to allow weakened criteria on the)J
54 550 :M
.814 .081(subsets we can provide the same performance guarantees)J
54 561 :M
(for these rule evaluation functions.)S
64 572 :M
.242 .024(In particular, given a set of examples, )J
220 572 :M
f3_10 sf
.129(E)A
f0_10 sf
.22 .022(, a partition of )J
f3_10 sf
(E)S
54 583 :M
f0_10 sf
(into )S
f3_10 sf
(N)S
79 583 :M
f0_10 sf
-.007( disjoint subsets, )A
f3_10 sf
(E)S
154 585 :M
(i)S
157 583 :M
f0_10 sf
(, )S
162 583 :M
f3_10 sf
(i=1..N)S
f0_10 sf
-.006(, and a secondary function)A
54 595 :M
f3_10 sf
.299(f'\(r,E,N\))A
f0_10 sf
.795 .079(, define a rule to be )J
176 595 :M
f3_10 sf
.177(acceptable)A
f0_10 sf
.621 .062( over an example)J
54 606 :M
.191 .019(subset, )J
f3_10 sf
(E)S
91 608 :M
(i)S
94 606 :M
f0_10 sf
.131 .013(, if )J
f3_10 sf
.1(f'\(r,E)A
129 608 :M
(i)S
132 606 :M
f0_10 sf
(,)S
135 606 :M
f3_10 sf
.399 .04(N\) )J
cF
f4_10 sf
.04<B3>A
sf
.399 .04( c)J
f0_10 sf
.189 .019(, )J
166 606 :M
f3_10 sf
.05(i.e.)A
f0_10 sf
.234 .023(, the rule is satisfactory with)J
54 618 :M
.578 .058(respect to )J
f3_10 sf
.214(f')A
102 618 :M
f0_10 sf
.835 .084(.  The )J
131 618 :M
f3_10 sf
.251 .025(extended )J
f0_10 sf
.382 .038(invariant-partitioning property)J
54 629 :M
(is the phenomenon that for some rule evaluation criteria the)S
54 640 :M
2.09 .209(following holds for all partitions of )J
218 640 :M
f3_10 sf
1.69(E)A
f0_10 sf
2.068 .207(: if a rule )J
f3_10 sf
(r)S
280 640 :M
f0_10 sf
2.816 .282( is)J
54 651 :M
1.104 .11(satisfactory over )J
127 651 :M
f3_10 sf
.689(E)A
f0_10 sf
1.323 .132(, then there exists an )J
228 651 :M
f3_10 sf
.391(i)A
f0_10 sf
1.434 .143( such that )J
277 651 :M
f3_10 sf
(r)S
281 651 :M
f0_10 sf
1.702 .17( is)J
54 662 :M
-.009(acceptable over )A
119 662 :M
f3_10 sf
(E)S
0 2 rm
(i)S
0 -2 rm
128 662 :M
f0_10 sf
-.01(. The usefulness of the extended property)A
54 674 :M
(hinges on the definition of )S
f3_10 sf
(f'.)S
64 685 :M
f0_10 sf
1.605 .161(The global performance guarantee with the extended)J
54 696 :M
1.228 .123(property is the same as with the original, namely, )J
271 696 :M
f3_10 sf
(every)S
319 63 :M
1.478 .148(rule that is satisfactory to a monolithic learner will be)J
319 74 :M
1.037 .104(acceptable to at least one distributed learner)J
510 74 :M
f0_10 sf
1.361 .136(.  With the)J
319 85 :M
1.793 .179(original property, a rule was acceptable only if it was)J
319 96 :M
1.799 .18(satisfactory.  A weaker definition of acceptability will)J
319 107 :M
.968 .097(allow more rules to be found by the distributed learners.)J
319 118 :M
.464 .046(Below we utilize cooperation to ensure that spurious rules)J
319 129 :M
.69 .069(are eliminated.  We now show that for a non-trivial )J
537 129 :M
f3_10 sf
(f')S
542 129 :M
f0_10 sf
.801 .08( the)J
319 140 :M
(extended property holds for the Laplace accuracy estimate.)S
329 151 :M
.423 .042(Define the )J
f3_10 sf
.677 .068(Laplace estimate criterion)J
f0_10 sf
.198 .02( as: )J
500 151 :M
f3_10 sf
(f\(r,E\)=le\(r,E\),)S
319 162 :M
(c=L)S
336 162 :M
f0_10 sf
13.591 1.359(. Define )J
409 162 :M
f3_10 sf
10.77 1.077(f'\(r,E,N\) = le'\(r,E,N\) =)J
319 173 :M
(\(TP+1/N\)/\(TP+FP+k/N\).)S
422 173 :M
f0_10 sf
-.019(  As expected, )A
481 173 :M
f3_10 sf
-.006(le'\(r,E,N\))A
f0_10 sf
( )S
521 173 :M
f1_10 sf
(@)S
f0_10 sf
( )S
529 173 :M
f3_10 sf
(le\(r,E\),)S
319 184 :M
f0_10 sf
2.031 .203(which means that the criterion used on the subsets is)J
319 195 :M
.386 .039(approximately the same as that used on the entire data set.)J
319 206 :M
-.011(In fact, it is easy to verify that for )A
f3_10 sf
-.029(N=1)A
474 206 :M
f0_10 sf
(, )S
f3_10 sf
-.008(le'\(r,E,N\) = le\(r,E\);)A
319 217 :M
.044 .004(as N)J
f4_10 sf
<AE>S
347 217 :M
f3_10 sf
.095 .009( )J
350 217 :M
f4_10 sf
.048 .005<A52C20>J
f0_10 sf
( )S
f3_10 sf
.104 .01(le'\(r,E,N\) )J
405 217 :M
f4_10 sf
<AE20>S
f3_10 sf
.008(ppv\(r,E,N\))A
460 217 :M
f0_10 sf
.035 .004(, and for )J
f3_10 sf
.099 .01(N>1, le'\(r,E,N\))J
319 228 :M
f0_10 sf
(is between )S
364 228 :M
f3_10 sf
(le\(r,E\) )S
393 228 :M
f0_10 sf
(and)S
f3_10 sf
( ppv\(r,E\).)S
329 239 :M
f0_10 sf
.565 .056(Assume that for a rule, )J
f3_10 sf
(r)S
431 239 :M
f0_10 sf
.125 .012(: )J
f3_10 sf
.69 .069(\(TP+1\)/\(TP+FP+k\) )J
cF
f4_10 sf
.069<B3>A
sf
.69 .069( L)J
f0_10 sf
.383 .038(, but,)J
319 250 :M
.298 .03(given a partition of )J
f3_10 sf
.192 .019(N )J
409 250 :M
f0_10 sf
.309 .031(subsets of )J
f3_10 sf
.139(E)A
f0_10 sf
.109 .011(: )J
464 250 :M
f4_10 sf
.164(")A
f3_10 sf
.328 .033(i, \(TP)J
0 2 rm
.11 .011(i )J
0 -2 rm
500 250 :M
.191 .019(+1/N\)/ \(TP)J
0 2 rm
(i)S
0 -2 rm
548 250 :M
.368 .037( +)J
319 262 :M
(FP)S
0 2 rm
(i)S
0 -2 rm
334 262 :M
(+k/N\) < L)S
f0_10 sf
( \(i.e., )S
f3_10 sf
(r)S
402 262 :M
f0_10 sf
( is not acceptable over any )S
511 262 :M
f3_10 sf
(E)S
0 2 rm
(i)S
0 -2 rm
520 262 :M
f0_10 sf
(\),  then:)S
319 274 :M
f3_10 sf
(1\))S
339 274 :M
f4_10 sf
(")S
f3_10 sf
(i {TP)S
367 276 :M
(i)S
370 274 :M
( + 1/N <  L \341 \(TP)S
0 2 rm
(i)S
0 -2 rm
441 274 :M
( + FP)S
465 276 :M
(i)S
468 274 :M
( + k/N\)})S
319 286 :M
(2\))S
339 286 :M
f4_10 sf
(S)S
345 286 :M
f3_10 sf
(\(TP)S
360 288 :M
(i)S
363 286 :M
( +1/N\) < )S
402 286 :M
f4_10 sf
(S)S
408 286 :M
f3_10 sf
(\(L \341 \(TP)S
0 2 rm
(i)S
0 -2 rm
442 286 :M
(+FP)S
461 288 :M
(i)S
464 286 :M
( + k/N\)\))S
319 298 :M
(3\))S
339 298 :M
f4_10 sf
(S)S
345 298 :M
f3_10 sf
(\(TP)S
360 300 :M
(i)S
363 298 :M
( +1/N\) < L \341 )S
415 298 :M
f4_10 sf
(S)S
421 298 :M
f3_10 sf
(\(TP)S
436 300 :M
(i)S
439 298 :M
(+FP)S
458 300 :M
(i)S
461 298 :M
( + k/N\))S
319 310 :M
f0_10 sf
(4\))S
339 310 :M
(TP + 1 < L \341  \(TP + FP + k\))S
319 321 :M
(5\))S
339 321 :M
(\(TP+1\)/\(TP+FP+k\)  < L   ==>   Contradiction)S
319 332 :M
1.465 .147(Furthermore, it can be shown that )J
f3_10 sf
.469(le')A
481 332 :M
f0_10 sf
1.777 .178( is tight; it is the)J
319 343 :M
2.786 .279(strongest function for which the extended invariant-)J
319 354 :M
2.793 .279(partitioning property will hold.  By using a similar)J
319 365 :M
1.158 .116(derivation, it is easy to show that the extended property)J
319 376 :M
1.665 .166(applies to the certainty factor used by Quinlan. It also)J
319 387 :M
2.277 .228(applies to the certainty factor normalized for skewed)J
319 398 :M
.759 .076(distributions.  Specifically, )J
434 398 :M
f3_10 sf
.772 .077(f\(r,E\) = cf_normalized\(r,E\) =)J
319 409 :M
(\(TP-0.5\)/\(TP+)S
378 409 :M
f4_10 sf
(r)S
f3_10 sf
(FP\))S
399 409 :M
f0_10 sf
3.353 .335(, where )J
440 409 :M
f4_10 sf
1.305(r)A
f0_10 sf
2.882 .288( is the ratio of positive)J
319 420 :M
(examples to negative examples in the training set.)S
351 452 :M
f2_12 sf
(Cooperating Distributed Learners)S
319 470 :M
f0_10 sf
1.874 .187(We have designed and implemented DRL \(Distributed)J
319 481 :M
4.303 .43(Rule Learner\) taking advantage of the invariant)J
555 481 :M
(-)S
319 492 :M
1.24 .124(partitioning property. DRL partitions and distributes the)J
319 503 :M
1.104 .11(examples across a network of conventional workstations)J
319 514 :M
1.292 .129(each running an instance of a rule learning program. In)J
319 525 :M
.02 .002(DRL the learners cooperate based on the communication of)J
319 536 :M
1.866 .187(partial results to each other. The invariant-partitioning)J
319 547 :M
.296 .03(property guarantees that any rule that is satisfactory on the)J
319 558 :M
1.174 .117(entire data set will be found by one of the sub-learners.)J
319 569 :M
3.038 .304(Simple cooperation assures that )J
470 569 :M
f3_10 sf
.976(only)A
f0_10 sf
2.79 .279( rules that are)J
319 580 :M
.509 .051(satisfactory on the entire data set will be found.  Later we)J
319 591 :M
(will discuss more elaborate cooperation.)S
319 615 :M
f2_11 sf
(DRL)S
319 630 :M
f0_10 sf
.305 .031(DRL is based upon RL \(Clearwater & Provost, 1990\).  RL)J
319 641 :M
4.535 .454(performs a general-to-specific beam search of a)J
319 652 :M
.238 .024(syntactically defined space of rules, similar to that of other)J
319 663 :M
.081 .008(MetaDENDRAL-style rule learners \(Buchanan & Mitchell,)J
319 674 :M
.922 .092(1978; Segal & Etzioni, 1994; Webb 1995\), for rules that)J
319 685 :M
1.167 .117(satisfy a user-defined rule evaluation criterion.  For this)J
319 696 :M
(work, we use )S
375 696 :M
f3_10 sf
(cf_normalized)S
f0_10 sf
( \(defined above\).)S
endp
%%Page: 3 3
%%BeginPageSetup
initializepage
(; page: 3 of 6)setjob
%%EndPageSetup
-30 -31 :T
gS 30 31 552 730 rC
64 63 :M
f0_10 sf
1.737 .174(DRL first partitions the training data into )J
251 63 :M
f3_10 sf
.477(N)A
f0_10 sf
1.292 .129( disjoint)J
54 74 :M
.185 .019(subsets, assigns each subset to a machine, and provides the)J
54 85 :M
.419 .042(infrastructure for communication when individual learners)J
54 96 :M
.095 .01(detect an acceptable rule. When a rule meets the evaluation)J
54 107 :M
.017 .002(criterion for a subset of the data, it becomes a )J
239 107 :M
f3_10 sf
(candidate)S
f0_10 sf
( for)S
54 118 :M
1.727 .173(meeting the evaluation criterion globally; the extended)J
54 129 :M
1.415 .141(invariant-partitioning property guarantees that each rule)J
54 140 :M
3.011 .301(that is satisfactory over the entire data set will be)J
54 151 :M
.312 .031(acceptable over at least one subset.  As a local copy of RL)J
54 162 :M
.882 .088(discovers an acceptable rule, it broadcasts the rule to the)J
54 173 :M
.715 .072(other machines to review its statistics over the rest of the)J
54 184 :M
2.914 .291(examples. If the rule meets the evaluation criterion)J
54 195 :M
.831 .083(globally, it is posted as a satisfactory rule. Otherwise, its)J
54 206 :M
(local statistics are replaced with the global statistics and the)S
54 217 :M
.939 .094(rule is made available to be further specialized. Initially,)J
54 228 :M
.305 .031(the review of acceptable rules has been implemented as an)J
54 239 :M
(additional process that examines the entire data set.)S
54 263 :M
f2_11 sf
(Empirical Demonstration)S
64 278 :M
f0_10 sf
.271 .027(We have been using a rule-learning program to discover)J
54 289 :M
.3 .03(potential indicators of fraudulent cellular telephone calling)J
54 300 :M
2.231 .223(behavior.  The training data are examples of cellular)J
54 311 :M
1.94 .194(telephone calls, each described by 31 attributes, some)J
54 322 :M
1.856 .186(numeric, some discrete with hundreds or thousands of)J
54 333 :M
1.752 .175(possible values. The data set used for the experiments)J
54 344 :M
1.05 .105(reported here comprises over 1,000,000 examples. High)J
290 344 :M
(-)S
54 355 :M
2.383 .238(probability indicators are used to generate subscriber)J
54 366 :M
.779 .078(behavior profilers for fraud detection.  We chose a set of)J
54 377 :M
(parameters that had been used in previous learning work on)S
54 388 :M
(the fraud data for monolithic RL as well as for DRL.)S
54 399 :M
f2_10 sf
1.485 .148(The invariant-partitioning property is observed.)J
273 399 :M
f0_10 sf
2.809 .281(  In)J
54 410 :M
3.331 .333(order to examine whether the invariant-partitioning)J
54 421 :M
.322 .032(property is indeed observed \(as the above theory predicts\),)J
54 432 :M
.678 .068(we examined the rules learned by the multiple processors)J
54 443 :M
2.195 .22(in runs of DRL using multiple processes on multiple)J
54 454 :M
.857 .086(workstations \(as described below\) and compared them to)J
54 465 :M
.129 .013(the rules learned by a monolithic RL using the union of the)J
54 476 :M
1.376 .138(DRL processors' data sets.  As expected, the individual)J
54 487 :M
1.222 .122(DRL processes learned different rule sets: some did not)J
54 498 :M
2.336 .234(find all the rules found by the monolithic RL; some)J
54 509 :M
1.648 .165(produced spurious rules that were not validated by the)J
54 520 :M
1.414 .141(global review.  However, as predicted by the invariant-)J
54 531 :M
.814 .081(partitioning property, the final rule set produced by DRL)J
54 542 :M
.654 .065(was essentially the same as the final rule set produced by)J
54 553 :M
.015 .001(the monolithic RL.  The only difference in the rule sets was)J
54 564 :M
.403 .04(that DRL found some extra, globally satisfactory rules not)J
54 575 :M
.853 .085(found by RL.  This is due to the fact that RL conducts a)J
54 586 :M
1.44 .144(heuristic \(beam\) search.  Because of the distribution of)J
54 597 :M
3.288 .329(examples across the subsets of the partition, some)J
54 608 :M
.689 .069(processors found rules that had fallen off the beam in the)J
54 619 :M
.872 .087(monolithic search.  Thus, the distributed version actually)J
54 630 :M
.306 .031(learned )J
f3_10 sf
.284 .028(more )J
110 630 :M
f0_10 sf
.249 .025(satisfactory rules than the monolithic version)J
54 641 :M
(in addition to learning substantially faster.)S
54 652 :M
f2_10 sf
1.74 .174(Scaling up.)J
105 652 :M
f0_10 sf
2.34 .234( Figure 1 shows the run times of several)J
54 663 :M
1.736 .174(different versions of the rule learner as the number of)J
54 674 :M
1.784 .178(examples increases: monolithic RL \(RL\), a semi-serial)J
54 685 :M
.325 .032(version of DRL, and DRL running on four processors plus)J
54 696 :M
.204 .02(a fifth for the rule review.  RL's run time increases linearly)J
319 63 :M
.21 .021(in the number of examples, until the example set no longer)J
319 74 :M
.835 .084(fits in main memory, at which point the learner thrashes,)J
319 85 :M
.815 .081(constantly paging the example set during matching.  It is)J
319 96 :M
.439 .044(possible to create a serial version of DRL that operates on)J
319 107 :M
.048 .005(the subsets one after the other on a single machine, in order)J
319 118 :M
.073 .007(to avoid memory-management problems when the example)J
319 129 :M
.842 .084(sets become large.  However, because it does not exhibit)J
319 140 :M
2.125 .213(true \(learning-time\) cooperation, there is a significant)J
319 151 :M
.128 .013(overhead involved with the further specialization of locally)J
319 162 :M
.317 .032(acceptable rules that are not globally satisfactory, which is)J
319 173 :M
4.181 .418(necessary to guarantee performance equivalent to)J
319 184 :M
.922 .092(monolithic RL. )J
386 184 :M
f3_10 sf
.171(Semi-serial-DRL)A
f0_10 sf
.588 .059( uses a second processor)J
319 195 :M
4.397 .44(for the rule review, thus avoiding much of the)J
319 206 :M
1.242 .124(aforementioned overhead.  Figure 1 also includes a line)J
319 217 :M
.383 .038(corresponding to five times the run time of DRL \(DRL*5\))J
319 228 :M
(to illustrate the efficiency of the distribution.)S
329 239 :M
.803 .08(For a fixed number of examples, the run time for each)J
319 250 :M
-.005(DRL processor does not change significantly as the number)A
319 261 :M
1.588 .159(of processors increases, suggesting that communication)J
319 272 :M
.856 .086(overhead is negligible.  For the DRL system used in this)J
319 283 :M
2.749 .275(demonstration, thrashing set in at just over 300,000)J
319 294 :M
(examples \(as expected\).)S
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
318 314 251 243 rC
-1 -1 363 520 1 1 362 328 @b
359 520 -1 1 366 519 1 359 519 @a
359 496 -1 1 366 495 1 359 495 @a
359 472 -1 1 366 471 1 359 471 @a
359 449 -1 1 366 448 1 359 448 @a
359 425 -1 1 366 424 1 359 424 @a
359 401 -1 1 366 400 1 359 400 @a
359 377 -1 1 366 376 1 359 376 @a
359 353 -1 1 366 352 1 359 352 @a
359 329 -1 1 366 328 1 359 328 @a
362 520 -1 1 548 519 1 362 519 @a
-1 -1 363 523 1 1 362 516 @b
-1 -1 426 523 1 1 425 516 @b
-1 -1 490 523 1 1 489 516 @b
357 323 196 201 rC
-1 -1 376 515 1 1 389 507 @b
-1 -1 390 508 1 1 402 500 @b
-1 -1 403 501 1 1 416 486 @b
-1 -1 417 487 1 1 429 475 @b
-1 -1 430 476 1 1 442 470 @b
-1 -1 443 471 1 1 456 465 @b
-1 -1 457 466 1 1 463 439 @b
-1 -1 464 440 1 1 469 348 @b
-1 -1 376 510 1 1 389 496 @b
-1 -1 390 497 1 1 402 488 @b
-1 -1 403 489 1 1 416 487 @b
-1 -1 417 488 1 1 429 471 @b
-1 -1 430 472 1 1 442 456 @b
-1 -1 443 457 1 1 456 452 @b
-1 -1 457 453 1 1 463 450 @b
463 451 -1 1 470 450 1 463 450 @a
-1 -1 470 451 1 1 483 438 @b
-1 -1 484 439 1 1 496 430 @b
-1 -1 376 512 1 1 389 500 @b
-1 -1 390 501 1 1 402 492 @b
-1 -1 403 493 1 1 416 491 @b
-1 -1 417 492 1 1 429 478 @b
-1 -1 430 479 1 1 442 466 @b
-1 -1 443 467 1 1 456 458 @b
456 459 -1 1 464 465 1 456 458 @a
-1 -1 464 466 1 1 469 458 @b
-1 -1 470 459 1 1 483 451 @b
-1 -1 484 452 1 1 496 444 @b
-1 -1 376 518 1 1 389 515 @b
-1 -1 390 516 1 1 402 514 @b
402 515 -1 1 417 514 1 402 514 @a
-1 -1 417 515 1 1 429 511 @b
-1 -1 430 512 1 1 442 509 @b
-1 -1 443 510 1 1 456 507 @b
456 508 -1 1 464 508 1 456 507 @a
-1 -1 464 509 1 1 469 507 @b
-1 -1 470 508 1 1 483 506 @b
-1 -1 484 507 1 1 496 504 @b
:e
0 G
372 511 6 6 rF
1 G
372.5 511.5 5 5 rS
0 G
386 504 6 6 rF
1 G
386.5 504.5 5 5 rS
0 G
399 497 6 6 rF
1 G
399.5 497.5 5 5 rS
0 G
413 483 6 6 rF
1 G
413.5 483.5 5 5 rS
0 G
426 472 6 6 rF
1 G
426.5 472.5 5 5 rS
0 G
439 467 6 6 rF
1 G
439.5 467.5 5 5 rS
0 G
453 462 6 6 rF
1 G
453.5 462.5 5 5 rS
0 G
460 436 6 6 rF
1 G
460.5 436.5 5 5 rS
0 G
466 345 6 6 rF
1 G
466.5 345.5 5 5 rS
372 506 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
372.5 506.5 5 5 rS
:e
1 G
386 493 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
386.5 493.5 5 5 rS
:e
1 G
399 485 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
399.5 485.5 5 5 rS
:e
1 G
413 484 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
413.5 484.5 5 5 rS
:e
1 G
426 468 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
426.5 468.5 5 5 rS
:e
1 G
439 453 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
439.5 453.5 5 5 rS
:e
1 G
453 449 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
453.5 449.5 5 5 rS
:e
1 G
460 447 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
460.5 447.5 5 5 rS
:e
1 G
466 447 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
466.5 447.5 5 5 rS
:e
1 G
480 435 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
480.5 435.5 5 5 rS
:e
1 G
493 427 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
493.5 427.5 5 5 rS
:e
0 G
np 375 514 :M
378 511 :L
375 508 :L
372 511 :L
375 514 :L
eofill
1 G
-1 -1 376 515 1 1 378 511 @b
375 509 -1 1 379 511 1 375 508 @a
-1 -1 373 512 1 1 375 508 @b
372 512 -1 1 376 514 1 372 511 @a
0 G
np 389 503 :M
392 500 :L
389 497 :L
386 500 :L
389 503 :L
eofill
1 G
-1 -1 390 504 1 1 392 500 @b
389 498 -1 1 393 500 1 389 497 @a
-1 -1 387 501 1 1 389 497 @b
386 501 -1 1 390 503 1 386 500 @a
0 G
np 402 495 :M
405 492 :L
402 489 :L
399 492 :L
402 495 :L
eofill
1 G
-1 -1 403 496 1 1 405 492 @b
402 490 -1 1 406 492 1 402 489 @a
-1 -1 400 493 1 1 402 489 @b
399 493 -1 1 403 495 1 399 492 @a
0 G
np 416 494 :M
419 491 :L
416 488 :L
413 491 :L
416 494 :L
eofill
1 G
-1 -1 417 495 1 1 419 491 @b
416 489 -1 1 420 491 1 416 488 @a
-1 -1 414 492 1 1 416 488 @b
413 492 -1 1 417 494 1 413 491 @a
0 G
np 429 481 :M
432 478 :L
429 475 :L
426 478 :L
429 481 :L
eofill
1 G
-1 -1 430 482 1 1 432 478 @b
429 476 -1 1 433 478 1 429 475 @a
-1 -1 427 479 1 1 429 475 @b
426 479 -1 1 430 481 1 426 478 @a
0 G
np 442 469 :M
445 466 :L
442 463 :L
439 466 :L
442 469 :L
eofill
1 G
-1 -1 443 470 1 1 445 466 @b
442 464 -1 1 446 466 1 442 463 @a
-1 -1 440 467 1 1 442 463 @b
439 467 -1 1 443 469 1 439 466 @a
0 G
np 456 461 :M
459 458 :L
456 455 :L
453 458 :L
456 461 :L
eofill
1 G
-1 -1 457 462 1 1 459 458 @b
456 456 -1 1 460 458 1 456 455 @a
-1 -1 454 459 1 1 456 455 @b
453 459 -1 1 457 461 1 453 458 @a
0 G
np 463 468 :M
466 465 :L
463 462 :L
460 465 :L
463 468 :L
eofill
1 G
-1 -1 464 469 1 1 466 465 @b
463 463 -1 1 467 465 1 463 462 @a
-1 -1 461 466 1 1 463 462 @b
460 466 -1 1 464 468 1 460 465 @a
0 G
np 469 461 :M
472 458 :L
469 455 :L
466 458 :L
469 461 :L
eofill
1 G
-1 -1 470 462 1 1 472 458 @b
469 456 -1 1 473 458 1 469 455 @a
-1 -1 467 459 1 1 469 455 @b
466 459 -1 1 470 461 1 466 458 @a
0 G
np 483 454 :M
486 451 :L
483 448 :L
480 451 :L
483 454 :L
eofill
1 G
-1 -1 484 455 1 1 486 451 @b
483 449 -1 1 487 451 1 483 448 @a
-1 -1 481 452 1 1 483 448 @b
480 452 -1 1 484 454 1 480 451 @a
0 G
np 496 447 :M
499 444 :L
496 441 :L
493 444 :L
496 447 :L
eofill
1 G
-1 -1 497 448 1 1 499 444 @b
496 442 -1 1 500 444 1 496 441 @a
-1 -1 494 445 1 1 496 441 @b
493 445 -1 1 497 447 1 493 444 @a
np 375 520 :M
378 517 :L
375 514 :L
372 517 :L
375 520 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 376 521 1 1 378 517 @b
375 515 -1 1 379 517 1 375 514 @a
-1 -1 373 518 1 1 375 514 @b
372 518 -1 1 376 520 1 372 517 @a
:e
1 G
np 389 518 :M
392 515 :L
389 512 :L
386 515 :L
389 518 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 390 519 1 1 392 515 @b
389 513 -1 1 393 515 1 389 512 @a
-1 -1 387 516 1 1 389 512 @b
386 516 -1 1 390 518 1 386 515 @a
:e
1 G
np 402 517 :M
405 514 :L
402 511 :L
399 514 :L
402 517 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 403 518 1 1 405 514 @b
402 512 -1 1 406 514 1 402 511 @a
-1 -1 400 515 1 1 402 511 @b
399 515 -1 1 403 517 1 399 514 @a
:e
1 G
np 416 517 :M
419 514 :L
416 511 :L
413 514 :L
416 517 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 417 518 1 1 419 514 @b
416 512 -1 1 420 514 1 416 511 @a
-1 -1 414 515 1 1 416 511 @b
413 515 -1 1 417 517 1 413 514 @a
:e
1 G
np 429 514 :M
432 511 :L
429 508 :L
426 511 :L
429 514 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 430 515 1 1 432 511 @b
429 509 -1 1 433 511 1 429 508 @a
-1 -1 427 512 1 1 429 508 @b
426 512 -1 1 430 514 1 426 511 @a
:e
1 G
np 442 512 :M
445 509 :L
442 506 :L
439 509 :L
442 512 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 443 513 1 1 445 509 @b
442 507 -1 1 446 509 1 442 506 @a
-1 -1 440 510 1 1 442 506 @b
439 510 -1 1 443 512 1 439 509 @a
:e
1 G
np 456 510 :M
459 507 :L
456 504 :L
453 507 :L
456 510 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 457 511 1 1 459 507 @b
456 505 -1 1 460 507 1 456 504 @a
-1 -1 454 508 1 1 456 504 @b
453 508 -1 1 457 510 1 453 507 @a
:e
1 G
np 463 511 :M
466 508 :L
463 505 :L
460 508 :L
463 511 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 464 512 1 1 466 508 @b
463 506 -1 1 467 508 1 463 505 @a
-1 -1 461 509 1 1 463 505 @b
460 509 -1 1 464 511 1 460 508 @a
:e
1 G
np 469 510 :M
472 507 :L
469 504 :L
466 507 :L
469 510 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 470 511 1 1 472 507 @b
469 505 -1 1 473 507 1 469 504 @a
-1 -1 467 508 1 1 469 504 @b
466 508 -1 1 470 510 1 466 507 @a
:e
1 G
np 483 509 :M
486 506 :L
483 503 :L
480 506 :L
483 509 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 484 510 1 1 486 506 @b
483 504 -1 1 487 506 1 483 503 @a
-1 -1 481 507 1 1 483 503 @b
480 507 -1 1 484 509 1 480 506 @a
:e
1 G
np 496 507 :M
499 504 :L
496 501 :L
493 504 :L
496 507 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 497 508 1 1 499 504 @b
496 502 -1 1 500 504 1 496 501 @a
-1 -1 494 505 1 1 496 501 @b
493 505 -1 1 497 507 1 493 504 @a
gR
gS 415 541 80 15 rC
417 551 :M
2.53 0 rm
f0_9 sf
(number of examples)S
gR
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
gS 30 31 552 730 rC
:e
gS
329 450 :T
270 rotate
-329 -450 :T
329 450 :M
0 G
f0_9 sf
.312 .031(run time \(sec\))J
gR
310 304 268 264 rC
349 522 :M
0 G
f0_9 sf
(0)S
336 498 :M
.667(1000)A
336 474 :M
.667(2000)A
336 450 :M
.667(3000)A
336 426 :M
.667(4000)A
336 402 :M
.667(5000)A
336 379 :M
.667(6000)A
336 355 :M
.667(7000)A
336 331 :M
.667(8000)A
360 536 :M
(0)S
414 536 :M
.625(50000)A
476 536 :M
.6(100000)A
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
487 322 70 15 rC
490 329 -1 1 539 328 1 490 328 @a
:e
0 G
511 325 6 6 rF
1 G
511.5 325.5 5 5 rS
541 332 :M
0 G
-.375(RL )A
gR
gS 487 343 76 37 rC
490 350 -1 1 539 349 1 490 349 @a
:e
1 G
511 346 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
511.5 346.5 5 5 rS
:e
541 353 :M
0 G
f0_9 sf
.252(semi-)A
541 364 :M
.303(serial)A
541 375 :M
-.499(DRL)A
gR
gS 487 386 82 15 rC
490 393 -1 1 539 392 1 490 392 @a
:e
0 G
np 514 395 :M
517 392 :L
514 389 :L
511 392 :L
514 395 :L
eofill
1 G
-1 -1 515 396 1 1 517 392 @b
514 390 -1 1 518 392 1 514 389 @a
-1 -1 512 393 1 1 514 389 @b
511 393 -1 1 515 395 1 511 392 @a
541 396 :M
0 G
f0_9 sf
(DRL*5)S
gR
gS 487 407 73 15 rC
490 414 -1 1 539 413 1 490 413 @a
:e
1 G
np 514 416 :M
517 413 :L
514 410 :L
511 413 :L
514 416 :L
eofill
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
-1 -1 515 417 1 1 517 413 @b
514 411 -1 1 518 413 1 514 410 @a
-1 -1 512 414 1 1 514 410 @b
511 414 -1 1 515 416 1 511 413 @a
:e
541 417 :M
0 G
f0_9 sf
-.499(DRL)A
gR
:e
gS 30 31 552 730 rC
321 577 :M
0 G
f2_10 sf
(Figure 1. Run time vs. number of examples for the)S
328 588 :M
(fraud data. \(averages over 3 runs\).  DRL uses 4)S
359 599 :M
(workstations + 1 for rule review.)S
319 618 :M
f0_10 sf
.996 .1(We are interested in the real-time expense of using such)J
319 629 :M
1.844 .184(systems, so these are real-time results, generated on a)J
319 640 :M
.983 .098(university laboratory network of DECstation 5000's with)J
319 651 :M
2.02 .202(32M of main memory.  Since the major non-linearity)J
319 662 :M
3.925 .392(hinges on the amount of main memory, we also)J
319 673 :M
.574 .057(experimented with dedicated Sparc10's with 64M of main)J
319 684 :M
.542 .054(memory.  For RL's run time, the shape of the graph is the)J
319 695 :M
1.46 .146(same.  Runs with 100,000 examples per processor take)J
endp
%%Page: 4 4
%%BeginPageSetup
initializepage
(; page: 4 of 6)setjob
%%EndPageSetup
-30 -31 :T
gS 30 31 552 730 rC
54 63 :M
f0_10 sf
.618 .062(approximately 20 minutes on the Sparc10s; thrashing sets)J
54 74 :M
.69 .069(in just under 300,000 examples.  This implies that with 5)J
54 85 :M
.789 .079(machines available, DRL can process a million examples)J
54 96 :M
(while you go get lunch.)S
64 107 :M
1.712 .171(The semi-serial version of DRL provides a practical)J
54 118 :M
(method for dealing with very large example sets even when)S
54 129 :M
3.873 .387(many processors are not available. The invariant)J
290 129 :M
(-)S
54 140 :M
3.984 .398(partitioning property allows it to make the same)J
54 151 :M
.533 .053(performance guarantees as DRL; the partitioning makes it)J
54 162 :M
.48 .048(very efficient by avoiding the scaling problems associated)J
54 173 :M
(with memory management.)S
54 197 :M
f2_11 sf
(Further cooperation)S
54 212 :M
f0_10 sf
3.414 .341(The study described above uses a simple form of)J
54 223 :M
.343 .034(cooperation to provide a guarantee of an output equivalent)J
54 234 :M
-.005(to that of a monolithic learner operating with the entire data)A
54 245 :M
.344 .034(set.  In this section we discuss three further ways in which)J
54 256 :M
.436 .044(cooperation can be used to benefit a set of distributed rule)J
54 267 :M
3.61 .361(learners.  Specifically, we discuss sharing learned)J
54 278 :M
1.232 .123(knowledge for \(i\) maximizing an accuracy estimate, \(ii\))J
54 289 :M
-.005(pruning portions of the rule space that are guaranteed not to)A
54 300 :M
.514 .051(contain satisfactory rules, and \(iii\) pruning portions of the)J
54 311 :M
(rule space heuristically.)S
64 322 :M
.553 .055(The invariant-partitioning property is based on learning)J
54 333 :M
.199 .02(rules whose evaluation function is greater than a threshold.)J
54 344 :M
.426 .043(Some existing rule learners search for rules that maximize)J
54 355 :M
1.092 .109(the positive predictive value \(Weiss, )J
f3_10 sf
.864 .086(et al.)J
233 355 :M
f0_10 sf
1.268 .127(, 1990\) or the)J
54 366 :M
1.34 .134(Laplace estimate \(Webb, 1995; Segal & Etzioni, 1994\).)J
54 377 :M
(DRL can approximate the maximization process by starting)S
54 388 :M
2.798 .28(with a high threshold and iteratively decreasing the)J
54 399 :M
1.505 .151(threshold if no rules are found.  However, a system of)J
54 410 :M
.97 .097(distributed learners can take advantage of cooperation to)J
54 421 :M
5.291 .529(maximize the rule evaluation function directly.)J
54 432 :M
1.003 .1(Specifically, each learner keeps track of the score of the)J
54 443 :M
.985 .098(globally best rule so far \(initially zero\).  When a learner)J
54 454 :M
.795 .08(finds a rule whose local evaluation exceeds the threshold)J
54 465 :M
.606 .061(defined by the global best, it sends the rule out for global)J
54 476 :M
.093 .009(review.  The invariant-partitioning property guarantees that)J
54 487 :M
.764 .076(the rule with the maximum global evaluation will exceed)J
54 498 :M
-.009(the global best-so-far on some processor. Initially there will)A
54 509 :M
.647 .065(be a flurry of communication, until a rule is found with a)J
54 520 :M
1.324 .132(large global evaluation. Communication will then occur)J
54 531 :M
(only if a learner finds a rule that exceeds this threshold.)S
64 542 :M
1.107 .111(Another benefit of cooperation is that one learner can)J
54 553 :M
.809 .081(reduce its search based on knowledge learned by another)J
54 564 :M
.322 .032(learner.  A thorough treatment of pruning for rule-learning)J
54 575 :M
.39 .039(search is beyond the scope of this paper, but Webb \(1995\))J
54 586 :M
.342 .034(discusses how massive portions of the search space can be)J
54 597 :M
.149 .015(pruned in the admissible search for the rule that maximizes)J
54 608 :M
.19 .019(the Laplace accuracy estimate.  In a distributed setting, if a)J
54 619 :M
.601 .06(learner discovers that a portion of the space is guaranteed)J
54 630 :M
-.014(not to contain satisfactory rules, it can share this knowledge)A
54 641 :M
2.315 .232(with the other learners.  Consider a simple, intuitive)J
54 652 :M
.571 .057(example: we are not interested in rules whose coverage is)J
54 663 :M
.687 .069(below a certain level.  When a learner finds a rule whose)J
54 674 :M
2.037 .204(coverage is below threshold, it sends the rule out for)J
54 685 :M
.278 .028(review.  If the review verifies that the rule is indeed below)J
54 696 :M
.24 .024(threshold globally, then the learner shares the rule with the)J
319 63 :M
.034 .003(group.  It is guaranteed that every specialization of this rule)J
319 74 :M
1.431 .143(will also be below threshold, so the portion of the rule)J
319 85 :M
.15 .015(space below this rule can be pruned.  Webb shows how the)J
319 96 :M
.86 .086(search space can be rearranged dynamically to maximize)J
319 107 :M
(the effect of pruning.)S
329 118 :M
3.848 .385(Cooperation can also be used to reduce search)J
319 129 :M
.957 .096(heuristically.  Rule-learning programs are used primarily)J
319 140 :M
2.33 .233(for two types of learning: \(i\) discovery of rules that)J
319 151 :M
.616 .062(individually are interesting to domain experts, )J
f3_10 sf
.151(e.g.,)A
530 151 :M
f0_10 sf
.931 .093( in the)J
319 162 :M
1.076 .108(fraud domain, and \(ii\) learning a disjunctive set of rules)J
319 173 :M
.089 .009(that are used to build a classifier, )J
f3_10 sf
.034(e.g.)A
469 173 :M
f0_10 sf
.095 .01(, a decision list \(Clark)J
319 184 :M
.187 .019(& Niblett, 1989\).  Often the basis for building classifiers is)J
319 195 :M
1.202 .12(the common "covering" heuristic: )J
f3_10 sf
1.316 .132(iteratively learn rules)J
319 206 :M
.37 .037(that cover at least one example not covered by the current)J
319 217 :M
.842 .084(rule set )J
f0_10 sf
1.666 .167(\(Michalski, )J
f3_10 sf
1.011 .101(et al.)J
427 217 :M
f0_10 sf
1.206 .121(, 1986; Clark & Niblett, 1989;)J
319 228 :M
.589 .059(Segal & Etzioni, 1994\).  Distributed learning systems can)J
319 239 :M
.267 .027(get much leverage from cooperation based on the covering)J
319 250 :M
1.354 .135(heuristic.  Specifically, as individual learners find good)J
319 261 :M
2.199 .22(rules they can share them with the group.  Allowing)J
319 272 :M
-.008(different learners to search the space in different orders will)A
319 283 :M
2.441 .244(increase the effect of the cooperation.  Consider the)J
319 294 :M
.623 .062(following extreme example: a large search space contains)J
319 305 :M
.493 .049(10 rules that together cover the example set, and there are)J
319 316 :M
-.005(10 distributed learners each of which starts its search with a)A
319 327 :M
.261 .026(different one of these rules.  In this case, after each learner)J
319 338 :M
.776 .078(searches 1 rule \(plus the subsequent review and sharing\),)J
319 349 :M
2.519 .252(the learning is complete.  We hypothesize that such)J
319 360 :M
2.684 .268(cooperation can lead to superlinear speedups over a)J
319 371 :M
.813 .081(monolithic learner \()J
402 371 :M
f3_10 sf
.174(cf.)A
f0_10 sf
.893 .089(, work on superlinear speedups for)J
319 382 :M
4.73 .473(constraint satisfaction problems \(Kornfeld, 1982;)J
319 393 :M
(Clearwater, )S
368 393 :M
f3_10 sf
(et al.)S
388 393 :M
f0_10 sf
(, 1991\)\).)S
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
318 413 225 243 rC
-1 -1 362 620 1 1 361 427 @b
358 620 -1 1 365 619 1 358 619 @a
358 600 -1 1 365 599 1 358 599 @a
358 581 -1 1 365 580 1 358 580 @a
358 562 -1 1 365 561 1 358 561 @a
358 543 -1 1 365 542 1 358 542 @a
358 524 -1 1 365 523 1 358 523 @a
358 505 -1 1 365 504 1 358 504 @a
358 485 -1 1 365 484 1 358 484 @a
358 466 -1 1 365 465 1 358 465 @a
358 447 -1 1 365 446 1 358 446 @a
358 428 -1 1 365 427 1 358 427 @a
361 620 -1 1 525 619 1 361 619 @a
-1 -1 362 623 1 1 361 616 @b
-1 -1 405 623 1 1 404 616 @b
-1 -1 448 623 1 1 447 616 @b
-1 -1 490 623 1 1 489 616 @b
356 422 174 202 rC
-1 -1 385 588 1 1 418 553 @b
-1 -1 419 554 1 1 429 533 @b
-1 -1 430 534 1 1 452 520 @b
-1 -1 453 521 1 1 474 497 @b
-1 -1 475 498 1 1 497 462 @b
-1 -1 385 611 1 1 418 594 @b
-1 -1 419 595 1 1 429 583 @b
-1 -1 430 584 1 1 452 575 @b
-1 -1 453 576 1 1 474 561 @b
-1 -1 475 562 1 1 497 543 @b
-1 -1 385 616 1 1 418 607 @b
-1 -1 419 608 1 1 429 605 @b
-1 -1 430 606 1 1 452 601 @b
-1 -1 453 602 1 1 474 583 @b
-1 -1 475 584 1 1 497 575 @b
:e
0 G
381 584 6 6 rF
1 G
381.5 584.5 5 5 rS
0 G
415 550 6 6 rF
1 G
415.5 550.5 5 5 rS
0 G
426 530 6 6 rF
1 G
426.5 530.5 5 5 rS
0 G
449 517 6 6 rF
1 G
449.5 517.5 5 5 rS
0 G
471 494 6 6 rF
1 G
471.5 494.5 5 5 rS
0 G
494 459 6 6 rF
1 G
494.5 459.5 5 5 rS
381 607 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
381.5 607.5 5 5 rS
:e
1 G
415 591 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
415.5 591.5 5 5 rS
:e
1 G
426 580 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
426.5 580.5 5 5 rS
:e
1 G
449 572 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
449.5 572.5 5 5 rS
:e
1 G
471 558 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
471.5 558.5 5 5 rS
:e
1 G
494 540 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
494.5 540.5 5 5 rS
:e
0 G
np 384 618 :M
387 615 :L
384 612 :L
381 615 :L
384 618 :L
eofill
1 G
-1 -1 385 619 1 1 387 615 @b
384 613 -1 1 388 615 1 384 612 @a
-1 -1 382 616 1 1 384 612 @b
381 616 -1 1 385 618 1 381 615 @a
0 G
np 418 610 :M
421 607 :L
418 604 :L
415 607 :L
418 610 :L
eofill
1 G
-1 -1 419 611 1 1 421 607 @b
418 605 -1 1 422 607 1 418 604 @a
-1 -1 416 608 1 1 418 604 @b
415 608 -1 1 419 610 1 415 607 @a
0 G
np 429 608 :M
432 605 :L
429 602 :L
426 605 :L
429 608 :L
eofill
1 G
-1 -1 430 609 1 1 432 605 @b
429 603 -1 1 433 605 1 429 602 @a
-1 -1 427 606 1 1 429 602 @b
426 606 -1 1 430 608 1 426 605 @a
0 G
np 452 604 :M
455 601 :L
452 598 :L
449 601 :L
452 604 :L
eofill
1 G
-1 -1 453 605 1 1 455 601 @b
452 599 -1 1 456 601 1 452 598 @a
-1 -1 450 602 1 1 452 598 @b
449 602 -1 1 453 604 1 449 601 @a
0 G
np 474 586 :M
477 583 :L
474 580 :L
471 583 :L
474 586 :L
eofill
1 G
-1 -1 475 587 1 1 477 583 @b
474 581 -1 1 478 583 1 474 580 @a
-1 -1 472 584 1 1 474 580 @b
471 584 -1 1 475 586 1 471 583 @a
0 G
np 497 578 :M
500 575 :L
497 572 :L
494 575 :L
497 578 :L
eofill
1 G
-1 -1 498 579 1 1 500 575 @b
497 573 -1 1 501 575 1 497 572 @a
-1 -1 495 576 1 1 497 572 @b
494 576 -1 1 498 578 1 494 575 @a
gR
gS 403 640 80 15 rC
405 650 :M
2.53 0 rm
f0_9 sf
(number of examples)S
gR
1 G
gS 30 31 552 730 rC
gS
329 549 :T
270 rotate
-329 -549 :T
329 549 :M
0 G
f0_9 sf
.312 .031(run time \(sec\))J
gR
310 403 240 264 rC
349 621 :M
0 G
f0_9 sf
(0)S
340 602 :M
.75(100)A
340 583 :M
.75(200)A
340 564 :M
.75(300)A
340 544 :M
.75(400)A
340 525 :M
.75(500)A
340 506 :M
.75(600)A
340 487 :M
.75(700)A
340 468 :M
.75(800)A
340 449 :M
.75(900)A
336 429 :M
.667(1000)A
359 635 :M
(0)S
393 635 :M
.625(20000)A
436 635 :M
.625(40000)A
478 635 :M
.625(60000)A
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
376 428 73 15 rC
379 435 -1 1 428 434 1 379 434 @a
:e
0 G
400 431 6 6 rF
1 G
400.5 431.5 5 5 rS
430 438 :M
0 G
-.499(DRL)A
gR
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
gS 376 449 73 26 rC
379 456 -1 1 428 455 1 379 455 @a
:e
1 G
400 452 6 6 rF
:a
0<FFFFFFFFFFFFFFFF><FFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFFF>fg null
:b
400.5 452.5 5 5 rS
:e
430 459 :M
0 G
f0_9 sf
-.499(DRL)A
430 470 :M
.502(/sc)A
gR
gS 376 481 73 26 rC
379 488 -1 1 428 487 1 379 487 @a
:e
0 G
np 403 490 :M
406 487 :L
403 484 :L
400 487 :L
403 490 :L
eofill
1 G
-1 -1 404 491 1 1 406 487 @b
403 485 -1 1 407 487 1 403 484 @a
-1 -1 401 488 1 1 403 484 @b
400 488 -1 1 404 490 1 400 487 @a
430 491 :M
0 G
f0_9 sf
-.499(DRL)A
430 502 :M
.255(/cc)A
gR
:e
gS 30 31 552 730 rC
315 676 :M
0 G
f2_10 sf
(Figure 2. The effect of the covering heuristic on  DRL)S
319 687 :M
(using 2 workstations + 1 for rule review. /sc denotes)S
323 698 :M
(simple covering. /cc denotes cooperative covering.)S
endp
%%Page: 5 5
%%BeginPageSetup
initializepage
(; page: 5 of 6)setjob
%%EndPageSetup
-30 -31 :T
gS 30 31 552 730 rC
64 63 :M
f0_10 sf
.77 .077(Figure 2 shows the effects of the covering heuristic on)J
54 74 :M
2.064 .206(the run time of DRL \(averages over 3 runs\).  Let us)J
54 85 :M
2.655 .265(distinguish between )J
147 85 :M
f3_10 sf
3.382 .338(simple covering \(/sc\))J
f0_10 sf
2.281 .228(, using the)J
54 96 :M
.408 .041(covering heuristic within a run of RL to cover an example)J
54 107 :M
1.319 .132(\(sub\)set, and )J
112 107 :M
f3_10 sf
.968 .097(cooperative covering \(/cc\))J
222 107 :M
f0_10 sf
1.21 .121(, sharing learned)J
54 118 :M
.281 .028(rules to cover the example set by the ensemble of learners.)J
54 129 :M
1.493 .149(As shown in the figure, for these runs simple covering)J
54 140 :M
.268 .027(provided approximately a factor of two speedup over DRL)J
54 151 :M
.825 .083(without covering. Cooperative covering provided another)J
54 162 :M
(factor of two speedup, on the average.)S
55 194 :M
f2_12 sf
( Related Work: Scaling Up Machine Learning)S
54 212 :M
f0_10 sf
1.929 .193(There are several approaches one might take to apply)J
54 223 :M
1.736 .174(symbolic machine learning to very large problems.  A)J
54 234 :M
.372 .037(straightforward, albeit limited, strategy for scaling up is to)J
54 245 :M
2.325 .233(use a fast, simple method. Holte \(1993\) showed that)J
54 256 :M
.24 .024(degenerate decision trees, )J
f3_10 sf
.33 .033(decision stumps)J
f0_10 sf
.246 .025(, performed well)J
54 267 :M
.794 .079(for many commonly used databases. While the algorithm)J
54 278 :M
.807 .081(for learning decision stumps is fast, the method prohibits)J
54 289 :M
(the learning of complex concepts.)S
64 300 :M
1.187 .119(A second strategy is to optimize a learning program's)J
54 311 :M
.452 .045(search and representation as much as possible, which may)J
54 322 :M
2.911 .291(involve the identification of constraints that can be)J
54 333 :M
1.366 .137(exploited to reduce algorithm complexity, or the use of)J
54 344 :M
1.399 .14(more efficient data structures \(Segal and Etzioni, 1994;)J
54 355 :M
.631 .063(Webb, 1995\). These techniques are complementary to the)J
54 366 :M
(scaling obtained by distributed processing.)S
64 377 :M
3.797 .38(The most common method for coping with the)J
54 388 :M
1.635 .163(infeasibility of learning from very large data sets is to)J
54 399 :M
1.054 .105(select a smaller sample from the initial data set.  Catlett)J
54 410 :M
.337 .034(\(1991b\) studied a variety of strategies for sampling from a)J
54 421 :M
.431 .043(large data set.  Despite the advantages of certain sampling)J
54 432 :M
.424 .042(strategies, Catlett concluded that they are not a solution to)J
54 443 :M
.428 .043(the problem of scaling up to very large data sets.  Fayyad,)J
54 454 :M
f3_10 sf
.145 .015(et al.)J
f0_10 sf
.192 .019( \(1993\), use sampling techniques, )J
213 454 :M
f3_10 sf
.214 .021(inter alia)J
f0_10 sf
.162 .016(, to reduce)J
54 465 :M
.125 .013(a huge data set \(over 3 terabytes of raw data\).  One method)J
54 476 :M
2.162 .216(they use is to partition the data set, learn rules from)J
54 487 :M
.702 .07(subsamples, and use a covering algorithm to combine the)J
54 498 :M
.166 .017(rules.  This method is similar to incremental batch learning)J
54 509 :M
2.944 .294(and coarse-grained parallel methods \(both described)J
54 520 :M
-.001(below\).  Catlett \(1991b; 1992\) also found that by looking at)A
54 531 :M
.801 .08(subsets when searching for good split values for numeric)J
54 542 :M
1.512 .151(attributes, the run time of decision-tree learners can be)J
54 553 :M
(reduced, without a corresponding loss in accuracy.)S
64 564 :M
1.75 .175(Incremental batch learning \(Clearwater, )J
f3_10 sf
1.153 .115(et al.)J
262 564 :M
f0_10 sf
1.967 .197(, 1989;)J
54 575 :M
.183 .018(Provost & Buchanan, 1995\), a cross between sampling and)J
54 586 :M
.238 .024(incremental learning, processes subsamples of examples in)J
54 597 :M
2.819 .282(sequence to learn from large training sets. Such an)J
54 608 :M
2.126 .213(approach is effective for scaling up because even for)J
54 619 :M
.069 .007(learners that scale up linearly in the number of examples, if)J
54 630 :M
1.351 .135(the example set does not fit in main memory, memory)J
290 630 :M
(-)S
54 641 :M
(management thrashing can render the learner useless.  Such)S
54 652 :M
1.085 .109(methods can take advantage of the invariant-partitioning)J
54 663 :M
1.714 .171(property and the covering heuristic to approximate the)J
54 674 :M
(effects of cooperation, as in a serial version of DRL.)S
64 685 :M
.495 .05(Gaines \(1989\) analyzed the extent that prior knowledge)J
54 696 :M
.778 .078(reduces the amount of data needed for effective learning.)J
319 63 :M
.68 .068(Unfortunately, pinpointing a small set of relevant domain)J
319 74 :M
1.432 .143(knowledge begs the very question of machine learning.)J
319 85 :M
(Aronis and Provost \(1994\) use parallelism to enable the use)S
319 96 :M
2.448 .245(of massive networks of domain knowledge to aid in)J
319 107 :M
(constructing new terms for inductive learning.)S
329 118 :M
4.118 .412(Finally, three approaches to decomposition and)J
319 129 :M
1.987 .199(parallelization can be identified.  First, in )J
f3_10 sf
.646(rule-space)A
319 140 :M
f0_10 sf
.443 .044(parallelization, the search of the rule space is decomposed)J
319 151 :M
.782 .078(such that different processors search different portions of)J
319 162 :M
2.711 .271(the rule space in parallel \(Cook and Holder, 1990\).)J
319 173 :M
.647 .065(However, this type of parallelization does not address the)J
319 184 :M
(problem of scaling up to very large data sets.)S
329 195 :M
.712 .071(The second parallelization approach, taken by Lathrop,)J
319 206 :M
f3_10 sf
1.254 .125(et al. )J
345 206 :M
f0_10 sf
.959 .096(\(1990\), and by Provost and Aronis \(1996\), utilizes)J
319 217 :M
f3_10 sf
.95 .095(parallel matching)J
f0_10 sf
.519 .052(, in which the example set is distributed)J
319 228 :M
.28 .028(to the processors of a massively parallel machine.  Provost)J
319 239 :M
.806 .081(and Aronis show that the parallel-matching approach can)J
319 250 :M
1.132 .113(scale a rule-learner up to millions of training data.  Our)J
319 261 :M
.181 .018(work differs from the massively parallel approaches in that)J
319 272 :M
.406 .041(our goal is to take advantage of existing \(and often under)J
554 272 :M
(-)S
319 283 :M
1.574 .157(utilized\) networked workstations, rather than expensive)J
319 294 :M
(parallel machines.)S
329 305 :M
2.623 .262(Finally, our work is best categorized by the third)J
319 316 :M
.138 .014(approach to parallel learning, the )J
455 316 :M
f3_10 sf
.01(coarse-grained)A
f0_10 sf
.049 .005( approach,)J
319 327 :M
1.515 .152(in which the data are divided among a set of powerful)J
319 338 :M
.853 .085(processors.  Each processor \(in parallel\) learns a concept)J
319 349 :M
2.102 .21(description from its set of examples, and the concept)J
319 360 :M
.326 .033(descriptions are combined.  Brazdil and Torgo \(1990\) take)J
319 371 :M
.036 .004(an approach similar to a distributed version of the approach)J
319 382 :M
.952 .095(of Fayyad, )J
f3_10 sf
.869 .087(et al.)J
389 382 :M
f0_10 sf
.859 .086(, \(described above\), in which a covering)J
319 393 :M
2.454 .245(algorithm is used to combine rules learned from the)J
319 404 :M
1.013 .101(subsets, but they do not experiment with very large data)J
319 415 :M
1.091 .109(sets.  Chan and Stolfo \(1993\) also take a coarse-grained)J
319 426 :M
.678 .068(approach and allow different learning programs to run on)J
319 437 :M
.716 .072(different processors. Not unexpectedly, as with sampling,)J
319 448 :M
2.778 .278(such techniques may degrade classification accuracy)J
319 459 :M
2.904 .29(compared to learning with the entire data set. This)J
319 470 :M
1.41 .141(degradation has been addressed by learning to combine)J
319 481 :M
1.697 .17(evidence from the several learned concept descriptions)J
319 492 :M
1.672 .167(\(Chan & Stolfo, 1994\). Our method differs from other)J
319 503 :M
2.053 .205(coarse-grained parallel learners \(and from incremental)J
319 514 :M
-.015(batch learning, and the method of Fayyad, )A
490 514 :M
f3_10 sf
-.017(et al.)A
510 514 :M
f0_10 sf
-.014(\), because it)A
319 525 :M
2.668 .267(utilizes cooperation between the distributed learners.)J
319 536 :M
4.202 .42(Cooperation allows guarantees to be made about)J
319 547 :M
.354 .035(performance of learned rules relative to the entire data set,)J
319 558 :M
2.35 .235(and can yield substantial speedups due to sharing of)J
319 569 :M
(learned knowledge.)S
410 601 :M
f2_12 sf
(Conclusion)S
319 619 :M
f0_10 sf
.892 .089(We demonstrate a powerful yet practical approach to the)J
319 630 :M
1.172 .117(use of parallel processing for addressing the problem of)J
319 641 :M
.9 .09(machine learning on very large data sets.  DRL does not)J
319 652 :M
.302 .03(require the use of expensive, highly specialized, massively)J
319 663 :M
2.186 .219(parallel hardware. Rather, it takes advantage of more)J
319 674 :M
1.071 .107(readily available, conventional hardware making it more)J
319 685 :M
4.221 .422(broadly applicable. Furthermore, DRL provides a)J
319 696 :M
(performance guarantee.)S
endp
%%Page: 6 6
%%BeginPageSetup
initializepage
(; page: 6 of 6)setjob
%%EndPageSetup
-30 -31 :T
gS 30 31 552 730 rC
64 63 :M
f0_10 sf
1.758 .176(Preliminary results indicate that we can scale up by)J
54 74 :M
2.201 .22(another order of magnitude by further optimizing the)J
54 85 :M
1.408 .141(search of the underlying learning system. For the fraud)J
54 96 :M
1.677 .168(data, a prototype system that uses spreading activation)J
54 107 :M
.668 .067(instead of matching as the basic learning operation learns)J
54 118 :M
2.551 .255(from 100,000 examples plus hierarchical background)J
54 129 :M
.559 .056(knowledge in under 5 minutes and 1,000,000 examples in)J
54 140 :M
-.006(five hours \(Aronis & Provost, 1996\).  This suggests that the)A
54 151 :M
3.086 .309(DRL system \(as configured above\) using spreading)J
54 162 :M
1.019 .102(activation instead of matching will learn from 1,000,000)J
54 173 :M
(examples in about an hour.)S
124 199 :M
f2_12 sf
(Acknowledgements)S
54 217 :M
f0_10 sf
2.772 .277(John Aronis has been involved in many stimulating)J
54 228 :M
.373 .037(discussions on scaling up machine learning.  The NYNEX)J
54 239 :M
1.416 .142(S & T Machine Learning Project and the University of)J
54 250 :M
(Pittsburgh Dept. of Medical Informatics provided support.)S
145 276 :M
f2_12 sf
(References)S
54 294 :M
f0_10 sf
3.178 .318(Aronis, J. M., & Provost, F. J. \(1994\). Efficiently)J
54 305 :M
3.858 .386(Constructing Relational Features from Background)J
54 316 :M
5.224 .522(Knowledge for Inductive Machine Learning. In)J
54 327 :M
f3_10 sf
(Proceedings of the AAAI-94 Workshop on KDD.)S
54 338 :M
f0_10 sf
3.625 .363(Aronis, J. & Provost, F. \(1996\). Using Spreading)J
54 349 :M
.476 .048(Activation for Increased Efficiency in Inductive Learning.)J
54 360 :M
.751 .075(Intelligent Systems Lab, Univ of Pittsburgh, Tech Report)J
54 371 :M
(ISL-96-7.)S
54 382 :M
.078 .008(Brazdil, P. & Torgo, L. \(1990\). Knowledge Acquisition via)J
54 393 :M
.506 .051(Knowledge Integration. In Wielinga \(ed.\), )J
230 393 :M
f3_10 sf
.401 .04(Current Trends)J
54 404 :M
(in Knowledge Acquisition,)S
160 404 :M
f0_10 sf
( 90-104. Amsterdam: IOS Press.)S
54 415 :M
2.345 .234(Buchanan, B., & Mitchell, T. \(1978\). Model-directed)J
54 426 :M
.156 .016(Learning of Production Rules. In Waterman & Hayes-Roth)J
54 437 :M
(\(ed.\), )S
78 437 :M
f3_10 sf
(Pattern Directed Inference Systems)S
f0_10 sf
(. Academic Press.)S
54 448 :M
2.636 .264(Catlett, J. \(1991a\). Megainduction: a Test Flight. In)J
54 459 :M
f3_10 sf
2.141 .214(Proceedings of the Eighth International Workshop on)J
54 470 :M
(Machine Learning)S
f0_10 sf
(,  p. 596-599. Morgan Kaufmann.)S
54 481 :M
.814 .081(Catlett, J. \(1991b\). )J
f3_10 sf
1.273 .127(Megainduction: machine learning on)J
54 492 :M
3.485 .349(very large databases)J
151 492 :M
f0_10 sf
3.997 .4(. Ph.D. Thesis, University of)J
54 503 :M
(Technology, Sydney.)S
54 514 :M
4.021 .402(Catlett, J. \(1992\). Peepholing: choosing attributes)J
54 525 :M
.448 .045(efficiently for megainduction. In )J
f3_10 sf
.469 .047(Proceedings of the Ninth)J
54 536 :M
(Int. Conf. on Machine Learning)S
f0_10 sf
(, 49-54. Morgan Kaufmann.)S
54 547 :M
2.816 .282(Chan, P., & Stolfo, S. \(1993\). Toward Parallel and)J
54 558 :M
.354 .035(Distributed Learning by Meta-Learning. In )J
232 558 :M
f3_10 sf
.281 .028(Proceedings of)J
54 569 :M
(the AAAI-93 Workshop on KDD)S
183 569 :M
f0_10 sf
(.)S
54 580 :M
(Chan, P., & Stolfo, S. \(1994\). Toward Scalable and Parallel)S
54 591 :M
2.31 .231(Inductive Learning: A Case Study in Splice Junction)J
54 602 :M
.479 .048(Prediction.  In the working notes of the ML-94 Workshop)J
54 613 :M
(on Machine Learning and Molecular Biology.)S
54 624 :M
.184 .018(Clark, P., & Boswell, R. \(1991\). Rule Induction with CN2:)J
54 635 :M
1.374 .137(Some recent improvements. In )J
189 635 :M
f3_10 sf
1.322 .132(Proceedings of the Fifth)J
54 646 :M
(European Working Session on Learning)S
215 646 :M
f0_10 sf
(,  p. 151-163.)S
54 657 :M
2.686 .269(Clark, P., & Niblett, T. \(1989\). The CN2 Induction)J
54 668 :M
(Algorithm. )S
101 668 :M
f3_10 sf
(Machine Learning)S
f0_10 sf
(, )S
f2_10 sf
(3)S
f0_10 sf
(, p. 261-283.)S
319 63 :M
1.947 .195(Clearwater, S., Cheng, T., Hirsh, H., & Buchanan, B.)J
319 74 :M
.35 .035(\(1989\). Incremental batch learning. In )J
477 74 :M
f3_10 sf
.41 .041(Proc. of the 6th Int.)J
319 85 :M
(Wkshp on Machine Learning)S
435 85 :M
f0_10 sf
(, 366-370. Morgan Kaufmann.)S
319 96 :M
3.201 .32(Clearwater, S., Huberman, B., & Hogg, T. \(1991\).)J
319 107 :M
.545 .054(Cooperative Solution of Constraint Satisfaction Problems.)J
319 118 :M
f3_10 sf
(Science)S
350 118 :M
f0_10 sf
(, )S
f2_10 sf
(254)S
f0_10 sf
(\(1991\), p. 1181-1183.)S
319 129 :M
1.6 .16(Clearwater, S., & Provost, F. \(1990\). RL4: A Tool for)J
319 140 :M
.21 .021(Knowledge-Based Induction. In )J
451 140 :M
f3_10 sf
.275 .028(Proc. of the 2nd Int. IEEE)J
319 151 :M
(Conf. on Tools for AI)S
f0_10 sf
(,  p. 24-30. IEEE C.S. Press.)S
319 162 :M
.938 .094(Cook, D., & Holder, L. \(1990\). Accelerated Learning on)J
319 173 :M
.678 .068(the Connection Machine. In )J
f3_10 sf
.603 .06(Proc. of the 2nd IEEE Symp.)J
319 184 :M
(on Parallel and Distributed Processing)S
477 184 :M
f0_10 sf
(,  p. 448-454.)S
319 195 :M
.622 .062(Fayyad, U., Weir, N., & Djorgovski, S. \(1993\). SKICAT:)J
319 206 :M
.407 .041(A Machine Learning System for Automated Cataloging of)J
319 217 :M
.628 .063(Large Scale Sky Surveys. In )J
440 217 :M
f3_10 sf
.62 .062(Proc. of the Tenth Int. Conf.)J
319 228 :M
(on Machine Learning)S
406 228 :M
f0_10 sf
(,  p. 112-119. Morgan Kaufmann.)S
319 239 :M
.415 .042(Gaines, B. R. \(1989\). An Ounce of Knowledge is Worth a)J
319 250 :M
2.12 .212(Ton of Data. In  )J
400 250 :M
f3_10 sf
1.786 .179(Proc. of the Sixth Int. Workshop on)J
319 261 :M
(Machine Learning)S
f0_10 sf
(,  p. 156-159. Morgan Kaufmann.)S
319 272 :M
2.455 .246(Holte, R. C. \(1993\). Very simple classification rules)J
319 283 :M
1.044 .104(perform well on most commonly used datasets. )J
523 283 :M
f3_10 sf
(Machine)S
319 294 :M
(Learning)S
356 294 :M
f0_10 sf
(, )S
f2_10 sf
(11)S
f0_10 sf
(\(1\), p. 63-90.)S
319 305 :M
3.169 .317(Kornfeld, W. A. \(1982\). Combinatorially Implosive)J
319 316 :M
(Algorithms. )S
370 316 :M
f3_10 sf
(Comm. of the ACM)S
f0_10 sf
(, )S
f2_10 sf
(25)S
f0_10 sf
(\(10\), p. 734-738.)S
319 327 :M
.062 .006(Lathrop, R. H., Webster, T. A., Smith, T. F., & Winston, P.)J
319 338 :M
3.323 .332(H. \(1990\). ARIEL: A Massively Parallel Symbolic)J
319 349 :M
.376 .038(Learning Assistant for Protein Structure/Function. In )J
f3_10 sf
.282 .028(AI at)J
319 360 :M
(MIT: Expanding Frontiers.)S
f0_10 sf
( Cambridge, MA: MIT Press.)S
319 371 :M
.526 .053(Michalski, R., Mozetic, I., Hong, J., & Lavrac, N. \(1986\).)J
319 382 :M
1.568 .157(The Multi-purpose Incremental Learning System AQ15)J
319 393 :M
.491 .049(and its Testing Application to Three Medical Domains. In)J
319 404 :M
f3_10 sf
(Proceedings of AAAI-86)S
417 404 :M
f0_10 sf
(,  p. 1041-1045. AAAI-Press.)S
319 415 :M
.889 .089(Provost, F. J., & Aronis, J. \(1996\). Scaling Up Inductive)J
319 426 :M
(Learning with Massive Parallelism. )S
464 426 :M
f3_10 sf
(Machine Learning)S
f0_10 sf
(, )S
f2_10 sf
(23)S
f0_10 sf
(.)S
319 437 :M
.108 .011(Provost, F. J., & Buchanan, B. G. \(1995\). Inductive Policy:)J
319 448 :M
2.366 .237(The Pragmatics of Bias Selection. )J
477 448 :M
f3_10 sf
1.461 .146(Machine Learning)J
f0_10 sf
(,)S
319 459 :M
f2_10 sf
(20)S
f0_10 sf
(\(1/2\), p. 35-61.)S
319 470 :M
.847 .085(Provost, F., & Hennessy, D. \(1994\). Distributed machine)J
319 481 :M
1.545 .154(learning: scaling up with coarse-grained parallelism. In)J
319 492 :M
f3_10 sf
1.41 .141(Proceedings of the Second International Conference on)J
319 503 :M
(Intelligent Systems for Molecular Biology)S
f0_10 sf
(.)S
319 514 :M
.751 .075(Quinlan, J. \(1986\). Induction of Decision Trees. )J
523 514 :M
f3_10 sf
(Machine)S
319 525 :M
(Learning)S
356 525 :M
f0_10 sf
(, )S
f2_10 sf
(1)S
f0_10 sf
(, p. 81-106.)S
319 536 :M
2.399 .24(Quinlan, J. \(1987\). Generating production rules from)J
319 547 :M
.775 .078(decision trees. In )J
f3_10 sf
1.282 .128(Proceedings of IJCAI-87)J
499 547 :M
f0_10 sf
1.281 .128(,  p. 304-307.)J
319 558 :M
(Morgan Kaufmann.)S
319 569 :M
4.003 .4(Quinlan, J. R., & Cameron-Jones, R. M. \(1995\).)J
319 580 :M
.44 .044(Oversearching and Layered Search in Empirical Learning.)J
319 591 :M
(In )S
330 591 :M
f3_10 sf
(Proc. of IJCAI-95)S
402 591 :M
f0_10 sf
(, 1019-1024. Morgan Kaufmann.)S
319 602 :M
.963 .096(Segal, R., & Etzioni, O. \(1994\). Learning Decision Lists)J
319 613 :M
.168 .017(using Homogeneous Rules. In )J
f3_10 sf
.221 .022(Proceedings of AAAI-94)J
542 613 :M
f0_10 sf
.284 .028(,  p.)J
319 624 :M
(619-625. AAAI Press.)S
319 635 :M
2.444 .244(Webb, G. I. \(1995\). OPUS: An Efficient Admissible)J
319 646 :M
2.135 .214(Algorithm for Unordered Search. )J
470 646 :M
f3_10 sf
1.895 .189(Journal of Artificial)J
319 657 :M
(Intelligence Research)S
406 657 :M
f0_10 sf
(, )S
f2_10 sf
(3)S
f0_10 sf
(, p. 431-465.)S
319 668 :M
1.539 .154(Weiss, S. M., Galen, R. S., & Tadepalli, P. V. \(1990\).)J
319 679 :M
1.847 .185(Maximizing the Predictive Value of Production Rules.)J
319 690 :M
f3_10 sf
(Artificial Intelligence)S
405 690 :M
f0_10 sf
(, )S
f2_10 sf
(45)S
f0_10 sf
(, p. 47-71.)S
endp
%%Trailer
end
%%EOF
